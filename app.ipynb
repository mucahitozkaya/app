{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mucahitozkaya/app/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7gO6SZQmiBYz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation,Dropout, BatchNormalization,MaxPool1D,LayerNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC  # Import Support Vector Classifier (SVC)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from logging import Logger\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read datas"
      ],
      "metadata": {
        "id": "rlEn2l9O1vR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = pd.read_parquet(\"https://drive.google.com/uc?id=1ZeXb2EBMkP-nzULonq_hDnIxF5u9PwKp\")\n",
        "test_final = pd.read_parquet(\"https://drive.google.com/uc?id=1Blepx7rAAUXFLLNIL8gCI2g1IcNoMfTk\")\n",
        "submission_sample = pd.read_parquet(\"https://drive.google.com/uc?id=1KCcrx84tmG5biO0MjLmnFhvCMSC-U6ge\")"
      ],
      "metadata": {
        "id": "HQOAxClkLfnY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_final\n",
        "df_test = test_final"
      ],
      "metadata": {
        "id": "ImiLMjFoP_3o"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
        "df['target_encoded'].nunique()"
      ],
      "metadata": {
        "id": "utk0ofGrRQcT",
        "outputId": "31aa58c8-bdb1-45a9-c059-cbba6dc4db3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_corr = df.loc[:, 'feature_0':'feature_24']\n",
        "features_corr['target_encoded'] = df['target_encoded']\n",
        "corr_matrix = features_corr.corr()\n",
        "\n",
        "plt.figure(figsize=(24, 18))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Sütunlar Arasındaki Korelasyon\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N9YfRr4XV_Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_corr = df.loc[:, 'feature_25':'target_encoded']\n",
        "features_corr = features_corr.drop(columns=['target'])\n",
        "corr_matrix = features_corr.corr()\n",
        "\n",
        "plt.figure(figsize=(24, 18))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Sütunlar Arasındaki Korelasyon\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EbeRWpRoR0EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target_cleaned'] = df['target'].apply(lambda x: ','.join([value.strip() for value in x.split(',')]))\n",
        "\n",
        "df[['target1', 'target2','target3']] = df['target_cleaned'].str.split(',', expand=True)\n",
        "\n",
        "df.drop(columns=['target','target_cleaned'], inplace=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "iMU3ZXfXrF6r",
        "outputId": "beb7252c-3d3c-4400-e545-8d92333c1c97"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              id  month  n_seconds_1  n_seconds_2  \\\n",
              "0      5beefd4d2bf4a4767e0df8108     10     5245.571      981.182   \n",
              "1      867285b116c063d5a8482f5be     10     5184.876      557.650   \n",
              "2      c82a7cbd2e00d9b66c06bcadc     10     3835.618     3275.128   \n",
              "3      f2d2b25073ccc298eced86897     10     3532.544      154.509   \n",
              "4      7818c92a58af0f2cb7c361738     10     3344.192      787.896   \n",
              "...                          ...    ...          ...          ...   \n",
              "94044  2e54f32ced9fae6ef802ceaa2     12       44.397       43.425   \n",
              "94045  6aa4ff0f6cc5ef4c2980b2862     12       44.331       43.977   \n",
              "94046  fa842185a0edd210845b78308     12       44.142       43.591   \n",
              "94047  db8f55b8499f8d8c05148240e     12       43.963       43.350   \n",
              "94048  93c3b8ecbb3071cf0a925a860     12       41.850       41.429   \n",
              "\n",
              "       n_seconds_3       carrier devicebrand  feature_0  feature_1  feature_2  \\\n",
              "0          205.948   VODAFONE TR       Apple  -1.197737   1.113360  -1.123334   \n",
              "1          487.587      TURKCELL     samsung  -2.336352   2.567766  -0.494908   \n",
              "2           43.806  TURK TELEKOM       Redmi  -2.561455   2.061736  -0.184511   \n",
              "3           64.724      TURKCELL     samsung  -2.529918   3.358050  -0.851366   \n",
              "4          715.115   VODAFONE TR     samsung  -2.922361   2.096124   0.060796   \n",
              "...            ...           ...         ...        ...        ...        ...   \n",
              "94044       41.678       O2 - DE      HUAWEI  -1.531534   2.596604   0.340233   \n",
              "94045       40.620      TURKCELL     samsung  -1.268987   2.300487   0.231711   \n",
              "94046       41.736   VODAFONE TR       Apple  -1.950039   2.805681   0.438200   \n",
              "94047       40.862  TURK TELEKOM       Redmi  -2.389140   2.358281   0.683524   \n",
              "94048       41.180      TURKCELL       HONOR  -2.372386   2.485552   0.824122   \n",
              "\n",
              "       ...  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
              "0      ...    2.645719   -1.023478    1.658986   -1.559406   -2.161336   \n",
              "1      ...   -0.021547   -0.195770    2.775513   -0.318980   -4.291473   \n",
              "2      ...    1.673868    0.631790    1.293131   -2.230909   -2.383524   \n",
              "3      ...    1.863495    0.213170    1.029710   -1.142185   -4.466191   \n",
              "4      ...    1.024499   -0.186423   -0.061626   -1.462175   -2.371206   \n",
              "...    ...         ...         ...         ...         ...         ...   \n",
              "94044  ...    0.044385    0.863442    0.761354    1.073711   -3.560019   \n",
              "94045  ...    2.406462   -0.340132   -0.893553   -2.061401   -1.908158   \n",
              "94046  ...    0.449427    0.445361   -0.495204   -0.955097   -3.843092   \n",
              "94047  ...    0.864857    1.284815   -0.100755   -1.056479   -1.452037   \n",
              "94048  ...    2.108022    0.175145    0.252684    0.528619   -2.203115   \n",
              "\n",
              "       feature_48  feature_49  target1  target2  target3  \n",
              "0              30          58    menu2    menu4    menu5  \n",
              "1              21          45    menu7    menu8    menu4  \n",
              "2              19          61    menu2    menu8    menu4  \n",
              "3               2          41    menu6    menu2    menu1  \n",
              "4              23          85    menu6    menu2    menu8  \n",
              "...           ...         ...      ...      ...      ...  \n",
              "94044          29          68    menu2    menu8    menu4  \n",
              "94045          25          43    menu9    menu2    menu5  \n",
              "94046           4          25    menu6    menu2    menu4  \n",
              "94047          25          60    menu6    menu8    menu4  \n",
              "94048          16          44    menu6    menu2    menu8  \n",
              "\n",
              "[94049 rows x 60 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9b18c3d-0d77-4899-b884-41e9d5dcf83d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>month</th>\n",
              "      <th>n_seconds_1</th>\n",
              "      <th>n_seconds_2</th>\n",
              "      <th>n_seconds_3</th>\n",
              "      <th>carrier</th>\n",
              "      <th>devicebrand</th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_43</th>\n",
              "      <th>feature_44</th>\n",
              "      <th>feature_45</th>\n",
              "      <th>feature_46</th>\n",
              "      <th>feature_47</th>\n",
              "      <th>feature_48</th>\n",
              "      <th>feature_49</th>\n",
              "      <th>target1</th>\n",
              "      <th>target2</th>\n",
              "      <th>target3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5beefd4d2bf4a4767e0df8108</td>\n",
              "      <td>10</td>\n",
              "      <td>5245.571</td>\n",
              "      <td>981.182</td>\n",
              "      <td>205.948</td>\n",
              "      <td>VODAFONE TR</td>\n",
              "      <td>Apple</td>\n",
              "      <td>-1.197737</td>\n",
              "      <td>1.113360</td>\n",
              "      <td>-1.123334</td>\n",
              "      <td>...</td>\n",
              "      <td>2.645719</td>\n",
              "      <td>-1.023478</td>\n",
              "      <td>1.658986</td>\n",
              "      <td>-1.559406</td>\n",
              "      <td>-2.161336</td>\n",
              "      <td>30</td>\n",
              "      <td>58</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu4</td>\n",
              "      <td>menu5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>867285b116c063d5a8482f5be</td>\n",
              "      <td>10</td>\n",
              "      <td>5184.876</td>\n",
              "      <td>557.650</td>\n",
              "      <td>487.587</td>\n",
              "      <td>TURKCELL</td>\n",
              "      <td>samsung</td>\n",
              "      <td>-2.336352</td>\n",
              "      <td>2.567766</td>\n",
              "      <td>-0.494908</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.021547</td>\n",
              "      <td>-0.195770</td>\n",
              "      <td>2.775513</td>\n",
              "      <td>-0.318980</td>\n",
              "      <td>-4.291473</td>\n",
              "      <td>21</td>\n",
              "      <td>45</td>\n",
              "      <td>menu7</td>\n",
              "      <td>menu8</td>\n",
              "      <td>menu4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c82a7cbd2e00d9b66c06bcadc</td>\n",
              "      <td>10</td>\n",
              "      <td>3835.618</td>\n",
              "      <td>3275.128</td>\n",
              "      <td>43.806</td>\n",
              "      <td>TURK TELEKOM</td>\n",
              "      <td>Redmi</td>\n",
              "      <td>-2.561455</td>\n",
              "      <td>2.061736</td>\n",
              "      <td>-0.184511</td>\n",
              "      <td>...</td>\n",
              "      <td>1.673868</td>\n",
              "      <td>0.631790</td>\n",
              "      <td>1.293131</td>\n",
              "      <td>-2.230909</td>\n",
              "      <td>-2.383524</td>\n",
              "      <td>19</td>\n",
              "      <td>61</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu8</td>\n",
              "      <td>menu4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f2d2b25073ccc298eced86897</td>\n",
              "      <td>10</td>\n",
              "      <td>3532.544</td>\n",
              "      <td>154.509</td>\n",
              "      <td>64.724</td>\n",
              "      <td>TURKCELL</td>\n",
              "      <td>samsung</td>\n",
              "      <td>-2.529918</td>\n",
              "      <td>3.358050</td>\n",
              "      <td>-0.851366</td>\n",
              "      <td>...</td>\n",
              "      <td>1.863495</td>\n",
              "      <td>0.213170</td>\n",
              "      <td>1.029710</td>\n",
              "      <td>-1.142185</td>\n",
              "      <td>-4.466191</td>\n",
              "      <td>2</td>\n",
              "      <td>41</td>\n",
              "      <td>menu6</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7818c92a58af0f2cb7c361738</td>\n",
              "      <td>10</td>\n",
              "      <td>3344.192</td>\n",
              "      <td>787.896</td>\n",
              "      <td>715.115</td>\n",
              "      <td>VODAFONE TR</td>\n",
              "      <td>samsung</td>\n",
              "      <td>-2.922361</td>\n",
              "      <td>2.096124</td>\n",
              "      <td>0.060796</td>\n",
              "      <td>...</td>\n",
              "      <td>1.024499</td>\n",
              "      <td>-0.186423</td>\n",
              "      <td>-0.061626</td>\n",
              "      <td>-1.462175</td>\n",
              "      <td>-2.371206</td>\n",
              "      <td>23</td>\n",
              "      <td>85</td>\n",
              "      <td>menu6</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94044</th>\n",
              "      <td>2e54f32ced9fae6ef802ceaa2</td>\n",
              "      <td>12</td>\n",
              "      <td>44.397</td>\n",
              "      <td>43.425</td>\n",
              "      <td>41.678</td>\n",
              "      <td>O2 - DE</td>\n",
              "      <td>HUAWEI</td>\n",
              "      <td>-1.531534</td>\n",
              "      <td>2.596604</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044385</td>\n",
              "      <td>0.863442</td>\n",
              "      <td>0.761354</td>\n",
              "      <td>1.073711</td>\n",
              "      <td>-3.560019</td>\n",
              "      <td>29</td>\n",
              "      <td>68</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu8</td>\n",
              "      <td>menu4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94045</th>\n",
              "      <td>6aa4ff0f6cc5ef4c2980b2862</td>\n",
              "      <td>12</td>\n",
              "      <td>44.331</td>\n",
              "      <td>43.977</td>\n",
              "      <td>40.620</td>\n",
              "      <td>TURKCELL</td>\n",
              "      <td>samsung</td>\n",
              "      <td>-1.268987</td>\n",
              "      <td>2.300487</td>\n",
              "      <td>0.231711</td>\n",
              "      <td>...</td>\n",
              "      <td>2.406462</td>\n",
              "      <td>-0.340132</td>\n",
              "      <td>-0.893553</td>\n",
              "      <td>-2.061401</td>\n",
              "      <td>-1.908158</td>\n",
              "      <td>25</td>\n",
              "      <td>43</td>\n",
              "      <td>menu9</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94046</th>\n",
              "      <td>fa842185a0edd210845b78308</td>\n",
              "      <td>12</td>\n",
              "      <td>44.142</td>\n",
              "      <td>43.591</td>\n",
              "      <td>41.736</td>\n",
              "      <td>VODAFONE TR</td>\n",
              "      <td>Apple</td>\n",
              "      <td>-1.950039</td>\n",
              "      <td>2.805681</td>\n",
              "      <td>0.438200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.449427</td>\n",
              "      <td>0.445361</td>\n",
              "      <td>-0.495204</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>-3.843092</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>menu6</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94047</th>\n",
              "      <td>db8f55b8499f8d8c05148240e</td>\n",
              "      <td>12</td>\n",
              "      <td>43.963</td>\n",
              "      <td>43.350</td>\n",
              "      <td>40.862</td>\n",
              "      <td>TURK TELEKOM</td>\n",
              "      <td>Redmi</td>\n",
              "      <td>-2.389140</td>\n",
              "      <td>2.358281</td>\n",
              "      <td>0.683524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.864857</td>\n",
              "      <td>1.284815</td>\n",
              "      <td>-0.100755</td>\n",
              "      <td>-1.056479</td>\n",
              "      <td>-1.452037</td>\n",
              "      <td>25</td>\n",
              "      <td>60</td>\n",
              "      <td>menu6</td>\n",
              "      <td>menu8</td>\n",
              "      <td>menu4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94048</th>\n",
              "      <td>93c3b8ecbb3071cf0a925a860</td>\n",
              "      <td>12</td>\n",
              "      <td>41.850</td>\n",
              "      <td>41.429</td>\n",
              "      <td>41.180</td>\n",
              "      <td>TURKCELL</td>\n",
              "      <td>HONOR</td>\n",
              "      <td>-2.372386</td>\n",
              "      <td>2.485552</td>\n",
              "      <td>0.824122</td>\n",
              "      <td>...</td>\n",
              "      <td>2.108022</td>\n",
              "      <td>0.175145</td>\n",
              "      <td>0.252684</td>\n",
              "      <td>0.528619</td>\n",
              "      <td>-2.203115</td>\n",
              "      <td>16</td>\n",
              "      <td>44</td>\n",
              "      <td>menu6</td>\n",
              "      <td>menu2</td>\n",
              "      <td>menu8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>94049 rows × 60 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9b18c3d-0d77-4899-b884-41e9d5dcf83d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9b18c3d-0d77-4899-b884-41e9d5dcf83d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9b18c3d-0d77-4899-b884-41e9d5dcf83d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9dc41001-ba40-416c-9f69-368e0b97234b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9dc41001-ba40-416c-9f69-368e0b97234b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9dc41001-ba40-416c-9f69-368e0b97234b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target1'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j0Yn6M1dH0l",
        "outputId": "a061eb47-f717-4b6f-c930-3d259e386333"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "menu6    59439\n",
              "menu9    15906\n",
              "menu2    13943\n",
              "menu3     2996\n",
              "menu7     1146\n",
              "menu8      367\n",
              "menu1      111\n",
              "menu4      105\n",
              "menu5       36\n",
              "Name: target1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target1'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjwMTtuLRxXX",
        "outputId": "7599c2a9-364e-4f7a-a6d7-2e9bf0c84a80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['menu2', 'menu7', 'menu6', 'menu9', 'menu3', 'menu8', 'menu1',\n",
              "       'menu4', 'menu5'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['id'].nunique()\n",
        "df.drop(columns=['carrier','devicebrand'], inplace=True)"
      ],
      "metadata": {
        "id": "PYnGCK96i3iZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSq8YEi8rOD3",
        "outputId": "f857a611-bb86-4c1a-f577-1f9d93e462ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 94049 entries, 0 to 94048\n",
            "Data columns (total 58 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   id           94049 non-null  object \n",
            " 1   month        94049 non-null  int64  \n",
            " 2   n_seconds_1  94049 non-null  float64\n",
            " 3   n_seconds_2  94049 non-null  float64\n",
            " 4   n_seconds_3  94049 non-null  float64\n",
            " 5   feature_0    94049 non-null  float64\n",
            " 6   feature_1    94049 non-null  float64\n",
            " 7   feature_2    94049 non-null  float64\n",
            " 8   feature_3    94049 non-null  float64\n",
            " 9   feature_4    94049 non-null  float64\n",
            " 10  feature_5    94049 non-null  float64\n",
            " 11  feature_6    94049 non-null  float64\n",
            " 12  feature_7    94049 non-null  float64\n",
            " 13  feature_8    94049 non-null  float64\n",
            " 14  feature_9    94049 non-null  float64\n",
            " 15  feature_10   94049 non-null  float64\n",
            " 16  feature_11   94049 non-null  float64\n",
            " 17  feature_12   94049 non-null  float64\n",
            " 18  feature_13   94049 non-null  float64\n",
            " 19  feature_14   94049 non-null  float64\n",
            " 20  feature_15   94049 non-null  float64\n",
            " 21  feature_16   94049 non-null  float64\n",
            " 22  feature_17   94049 non-null  float64\n",
            " 23  feature_18   94049 non-null  float64\n",
            " 24  feature_19   94049 non-null  float64\n",
            " 25  feature_20   94049 non-null  float64\n",
            " 26  feature_21   94049 non-null  float64\n",
            " 27  feature_22   94049 non-null  float64\n",
            " 28  feature_23   94049 non-null  float64\n",
            " 29  feature_24   94049 non-null  float64\n",
            " 30  feature_25   94049 non-null  float64\n",
            " 31  feature_26   94049 non-null  float64\n",
            " 32  feature_27   94049 non-null  float64\n",
            " 33  feature_28   94049 non-null  float64\n",
            " 34  feature_29   94049 non-null  float64\n",
            " 35  feature_30   94049 non-null  float64\n",
            " 36  feature_31   94049 non-null  float64\n",
            " 37  feature_32   94049 non-null  float64\n",
            " 38  feature_33   94049 non-null  float64\n",
            " 39  feature_34   94049 non-null  float64\n",
            " 40  feature_35   94049 non-null  float64\n",
            " 41  feature_36   94049 non-null  float64\n",
            " 42  feature_37   94049 non-null  float64\n",
            " 43  feature_38   94049 non-null  float64\n",
            " 44  feature_39   94049 non-null  float64\n",
            " 45  feature_40   94049 non-null  float64\n",
            " 46  feature_41   94049 non-null  float64\n",
            " 47  feature_42   94049 non-null  float64\n",
            " 48  feature_43   94049 non-null  float64\n",
            " 49  feature_44   94049 non-null  float64\n",
            " 50  feature_45   94049 non-null  float64\n",
            " 51  feature_46   94049 non-null  float64\n",
            " 52  feature_47   94049 non-null  float64\n",
            " 53  feature_48   94049 non-null  int64  \n",
            " 54  feature_49   94049 non-null  int64  \n",
            " 55  target1      94049 non-null  object \n",
            " 56  target2      94049 non-null  object \n",
            " 57  target3      94049 non-null  object \n",
            "dtypes: float64(51), int64(3), object(4)\n",
            "memory usage: 41.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Call Back"
      ],
      "metadata": {
        "id": "bXbIGKf_3t-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import datetime\n",
        "\n",
        "# eğitim sırasında en iyi model ağırlıklarını kayıt edelim ve\n",
        "# eğitim performansını TensorBoard ile izleyelim\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:04f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "logdir=\"/content/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "callbacks_list = [checkpoint,tensorboard_callback]\n"
      ],
      "metadata": {
        "id": "bVApzlL93Mc5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess"
      ],
      "metadata": {
        "id": "5fxeu5A_4SM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['id'] = label_encoder.fit_transform(df['id'])\n",
        "df['target1'] = label_encoder.fit_transform(df['target1'])\n",
        "df['target2'] = label_encoder.fit_transform(df['target2'])\n",
        "df['target3'] = label_encoder.fit_transform(df['target3'])\n",
        "\n"
      ],
      "metadata": {
        "id": "hzZy9JuGCvkE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target3'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEYnrKDUdXo2",
        "outputId": "bfaa7c2a-7267-4a41-f183-7f34987babaf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    26029\n",
              "6    19460\n",
              "3    18868\n",
              "0    16538\n",
              "7     5905\n",
              "4     4236\n",
              "1     2836\n",
              "5      177\n",
              "Name: target3, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWMQUSqHnfns",
        "outputId": "f93d558c-9d9b-40d5-f812-f0eff038861d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 94049 entries, 0 to 94048\n",
            "Data columns (total 58 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   id           94049 non-null  int64  \n",
            " 1   month        94049 non-null  int64  \n",
            " 2   n_seconds_1  94049 non-null  float64\n",
            " 3   n_seconds_2  94049 non-null  float64\n",
            " 4   n_seconds_3  94049 non-null  float64\n",
            " 5   feature_0    94049 non-null  float64\n",
            " 6   feature_1    94049 non-null  float64\n",
            " 7   feature_2    94049 non-null  float64\n",
            " 8   feature_3    94049 non-null  float64\n",
            " 9   feature_4    94049 non-null  float64\n",
            " 10  feature_5    94049 non-null  float64\n",
            " 11  feature_6    94049 non-null  float64\n",
            " 12  feature_7    94049 non-null  float64\n",
            " 13  feature_8    94049 non-null  float64\n",
            " 14  feature_9    94049 non-null  float64\n",
            " 15  feature_10   94049 non-null  float64\n",
            " 16  feature_11   94049 non-null  float64\n",
            " 17  feature_12   94049 non-null  float64\n",
            " 18  feature_13   94049 non-null  float64\n",
            " 19  feature_14   94049 non-null  float64\n",
            " 20  feature_15   94049 non-null  float64\n",
            " 21  feature_16   94049 non-null  float64\n",
            " 22  feature_17   94049 non-null  float64\n",
            " 23  feature_18   94049 non-null  float64\n",
            " 24  feature_19   94049 non-null  float64\n",
            " 25  feature_20   94049 non-null  float64\n",
            " 26  feature_21   94049 non-null  float64\n",
            " 27  feature_22   94049 non-null  float64\n",
            " 28  feature_23   94049 non-null  float64\n",
            " 29  feature_24   94049 non-null  float64\n",
            " 30  feature_25   94049 non-null  float64\n",
            " 31  feature_26   94049 non-null  float64\n",
            " 32  feature_27   94049 non-null  float64\n",
            " 33  feature_28   94049 non-null  float64\n",
            " 34  feature_29   94049 non-null  float64\n",
            " 35  feature_30   94049 non-null  float64\n",
            " 36  feature_31   94049 non-null  float64\n",
            " 37  feature_32   94049 non-null  float64\n",
            " 38  feature_33   94049 non-null  float64\n",
            " 39  feature_34   94049 non-null  float64\n",
            " 40  feature_35   94049 non-null  float64\n",
            " 41  feature_36   94049 non-null  float64\n",
            " 42  feature_37   94049 non-null  float64\n",
            " 43  feature_38   94049 non-null  float64\n",
            " 44  feature_39   94049 non-null  float64\n",
            " 45  feature_40   94049 non-null  float64\n",
            " 46  feature_41   94049 non-null  float64\n",
            " 47  feature_42   94049 non-null  float64\n",
            " 48  feature_43   94049 non-null  float64\n",
            " 49  feature_44   94049 non-null  float64\n",
            " 50  feature_45   94049 non-null  float64\n",
            " 51  feature_46   94049 non-null  float64\n",
            " 52  feature_47   94049 non-null  float64\n",
            " 53  feature_48   94049 non-null  int64  \n",
            " 54  feature_49   94049 non-null  int64  \n",
            " 55  target1      94049 non-null  int64  \n",
            " 56  target2      94049 non-null  int64  \n",
            " 57  target3      94049 non-null  int64  \n",
            "dtypes: float64(51), int64(7)\n",
            "memory usage: 41.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Katogorik verilerin one hot encoding yapılarak numerik hale getirilmesi\n",
        "object_list = []\n",
        "for column in df.columns:\n",
        "     if df[column].dtype == 'object':\n",
        "        object_list.append(column)\n",
        "try:\n",
        "  object_list.remove('id')\n",
        "  df = df.drop(columns=['id'])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "df1 = pd.get_dummies(df, columns=object_list)\n",
        "\n",
        "df1"
      ],
      "metadata": {
        "id": "Wa5txlw44WL3",
        "outputId": "a9e7c97c-9430-4527-8086-979c5d595fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       month  n_seconds_1  feature_0  feature_1  feature_2  feature_3  \\\n",
              "0         10     5245.571  -1.197737   1.113360  -1.123334  -0.263580   \n",
              "1         10     5184.876  -2.336352   2.567766  -0.494908   0.949101   \n",
              "2         10     3835.618  -2.561455   2.061736  -0.184511   1.062306   \n",
              "3         10     3532.544  -2.529918   3.358050  -0.851366   1.643876   \n",
              "4         10     3344.192  -2.922361   2.096124   0.060796  -1.487557   \n",
              "...      ...          ...        ...        ...        ...        ...   \n",
              "94044     12       44.397  -1.531534   2.596604   0.340233  -1.149720   \n",
              "94045     12       44.331  -1.268987   2.300487   0.231711   0.741582   \n",
              "94046     12       44.142  -1.950039   2.805681   0.438200   2.976427   \n",
              "94047     12       43.963  -2.389140   2.358281   0.683524   0.234449   \n",
              "94048     12       41.850  -2.372386   2.485552   0.824122   0.090134   \n",
              "\n",
              "       feature_4  feature_5  feature_6  feature_7  ...  devicebrand_iBRIT  \\\n",
              "0       2.161242   2.651375   0.810021   1.516175  ...                  0   \n",
              "1       3.567557   3.357848   0.434091   0.885814  ...                  0   \n",
              "2       4.197788   1.551181  -0.596218  -0.618501  ...                  0   \n",
              "3       2.849205   3.887427   1.854521   0.988186  ...                  0   \n",
              "4       3.224788   2.091947  -0.992961   0.686043  ...                  0   \n",
              "...          ...        ...        ...        ...  ...                ...   \n",
              "94044   3.308406   2.995904   1.117685   0.971643  ...                  0   \n",
              "94045   2.365813   2.031927   1.195348   0.209732  ...                  0   \n",
              "94046   4.001829   3.051181   0.431400   1.755406  ...                  0   \n",
              "94047   4.070453   1.709853   1.140759   0.187314  ...                  0   \n",
              "94048   3.370266   3.144336   0.112888   0.264923  ...                  0   \n",
              "\n",
              "       devicebrand_lge  devicebrand_meizu  devicebrand_motorola  \\\n",
              "0                    0                  0                     0   \n",
              "1                    0                  0                     0   \n",
              "2                    0                  0                     0   \n",
              "3                    0                  0                     0   \n",
              "4                    0                  0                     0   \n",
              "...                ...                ...                   ...   \n",
              "94044                0                  0                     0   \n",
              "94045                0                  0                     0   \n",
              "94046                0                  0                     0   \n",
              "94047                0                  0                     0   \n",
              "94048                0                  0                     0   \n",
              "\n",
              "       devicebrand_nubia  devicebrand_realme  devicebrand_reeder  \\\n",
              "0                      0                   0                   0   \n",
              "1                      0                   0                   0   \n",
              "2                      0                   0                   0   \n",
              "3                      0                   0                   0   \n",
              "4                      0                   0                   0   \n",
              "...                  ...                 ...                 ...   \n",
              "94044                  0                   0                   0   \n",
              "94045                  0                   0                   0   \n",
              "94046                  0                   0                   0   \n",
              "94047                  0                   0                   0   \n",
              "94048                  0                   0                   0   \n",
              "\n",
              "       devicebrand_samsung  devicebrand_vivo  devicebrand_xiaomi  \n",
              "0                        0                 0                   0  \n",
              "1                        1                 0                   0  \n",
              "2                        0                 0                   0  \n",
              "3                        1                 0                   0  \n",
              "4                        1                 0                   0  \n",
              "...                    ...               ...                 ...  \n",
              "94044                    0                 0                   0  \n",
              "94045                    1                 0                   0  \n",
              "94046                    0                 0                   0  \n",
              "94047                    0                 0                   0  \n",
              "94048                    0                 0                   0  \n",
              "\n",
              "[94049 rows x 673 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f49223b-bebc-4d65-b628-0b0aa8ae7be9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>n_seconds_1</th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>...</th>\n",
              "      <th>devicebrand_iBRIT</th>\n",
              "      <th>devicebrand_lge</th>\n",
              "      <th>devicebrand_meizu</th>\n",
              "      <th>devicebrand_motorola</th>\n",
              "      <th>devicebrand_nubia</th>\n",
              "      <th>devicebrand_realme</th>\n",
              "      <th>devicebrand_reeder</th>\n",
              "      <th>devicebrand_samsung</th>\n",
              "      <th>devicebrand_vivo</th>\n",
              "      <th>devicebrand_xiaomi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>5245.571</td>\n",
              "      <td>-1.197737</td>\n",
              "      <td>1.113360</td>\n",
              "      <td>-1.123334</td>\n",
              "      <td>-0.263580</td>\n",
              "      <td>2.161242</td>\n",
              "      <td>2.651375</td>\n",
              "      <td>0.810021</td>\n",
              "      <td>1.516175</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>5184.876</td>\n",
              "      <td>-2.336352</td>\n",
              "      <td>2.567766</td>\n",
              "      <td>-0.494908</td>\n",
              "      <td>0.949101</td>\n",
              "      <td>3.567557</td>\n",
              "      <td>3.357848</td>\n",
              "      <td>0.434091</td>\n",
              "      <td>0.885814</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>3835.618</td>\n",
              "      <td>-2.561455</td>\n",
              "      <td>2.061736</td>\n",
              "      <td>-0.184511</td>\n",
              "      <td>1.062306</td>\n",
              "      <td>4.197788</td>\n",
              "      <td>1.551181</td>\n",
              "      <td>-0.596218</td>\n",
              "      <td>-0.618501</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>3532.544</td>\n",
              "      <td>-2.529918</td>\n",
              "      <td>3.358050</td>\n",
              "      <td>-0.851366</td>\n",
              "      <td>1.643876</td>\n",
              "      <td>2.849205</td>\n",
              "      <td>3.887427</td>\n",
              "      <td>1.854521</td>\n",
              "      <td>0.988186</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>3344.192</td>\n",
              "      <td>-2.922361</td>\n",
              "      <td>2.096124</td>\n",
              "      <td>0.060796</td>\n",
              "      <td>-1.487557</td>\n",
              "      <td>3.224788</td>\n",
              "      <td>2.091947</td>\n",
              "      <td>-0.992961</td>\n",
              "      <td>0.686043</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94044</th>\n",
              "      <td>12</td>\n",
              "      <td>44.397</td>\n",
              "      <td>-1.531534</td>\n",
              "      <td>2.596604</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>-1.149720</td>\n",
              "      <td>3.308406</td>\n",
              "      <td>2.995904</td>\n",
              "      <td>1.117685</td>\n",
              "      <td>0.971643</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94045</th>\n",
              "      <td>12</td>\n",
              "      <td>44.331</td>\n",
              "      <td>-1.268987</td>\n",
              "      <td>2.300487</td>\n",
              "      <td>0.231711</td>\n",
              "      <td>0.741582</td>\n",
              "      <td>2.365813</td>\n",
              "      <td>2.031927</td>\n",
              "      <td>1.195348</td>\n",
              "      <td>0.209732</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94046</th>\n",
              "      <td>12</td>\n",
              "      <td>44.142</td>\n",
              "      <td>-1.950039</td>\n",
              "      <td>2.805681</td>\n",
              "      <td>0.438200</td>\n",
              "      <td>2.976427</td>\n",
              "      <td>4.001829</td>\n",
              "      <td>3.051181</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>1.755406</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94047</th>\n",
              "      <td>12</td>\n",
              "      <td>43.963</td>\n",
              "      <td>-2.389140</td>\n",
              "      <td>2.358281</td>\n",
              "      <td>0.683524</td>\n",
              "      <td>0.234449</td>\n",
              "      <td>4.070453</td>\n",
              "      <td>1.709853</td>\n",
              "      <td>1.140759</td>\n",
              "      <td>0.187314</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94048</th>\n",
              "      <td>12</td>\n",
              "      <td>41.850</td>\n",
              "      <td>-2.372386</td>\n",
              "      <td>2.485552</td>\n",
              "      <td>0.824122</td>\n",
              "      <td>0.090134</td>\n",
              "      <td>3.370266</td>\n",
              "      <td>3.144336</td>\n",
              "      <td>0.112888</td>\n",
              "      <td>0.264923</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>94049 rows × 673 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f49223b-bebc-4d65-b628-0b0aa8ae7be9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f49223b-bebc-4d65-b628-0b0aa8ae7be9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f49223b-bebc-4d65-b628-0b0aa8ae7be9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3352d1f7-cecf-4c3c-8ac5-f40a7958ad91\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3352d1f7-cecf-4c3c-8ac5-f40a7958ad91')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3352d1f7-cecf-4c3c-8ac5-f40a7958ad91 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_test['id'] = label_encoder.fit_transform(df_test['id'])\n",
        "df_test.drop(columns=['carrier','devicebrand'], inplace=True)"
      ],
      "metadata": {
        "id": "U6n_p7anjkyP"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "JaqG2D_nmfCp",
        "outputId": "6b7ed4d2-a1f0-473c-ee02-2010a520e4f1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  month  n_seconds_1  n_seconds_2  n_seconds_3  feature_0  \\\n",
              "0      2155      1     6893.544      246.854      242.636  -1.723524   \n",
              "1      9211      1     4481.065      740.209      263.860  -0.417275   \n",
              "2      3609      1     4340.702     2742.163      318.700  -2.943294   \n",
              "3      2440      1     4129.666      181.397      155.423  -2.346902   \n",
              "4        91      1     3903.944      126.133      100.060  -1.745354   \n",
              "...     ...    ...          ...          ...          ...        ...   \n",
              "11950  5559      1       47.726       40.879       40.647  -2.952961   \n",
              "11951  4508      1       47.471       44.745       41.040  -1.945921   \n",
              "11952  8094      1       47.403       45.832       43.932  -1.596234   \n",
              "11953  5891      1       46.044       43.592       40.735  -1.984582   \n",
              "11954  4123      1       45.802       45.056       40.199  -0.849872   \n",
              "\n",
              "       feature_1  feature_2  feature_3  feature_4  ...  feature_40  \\\n",
              "0       3.216489  -1.138474   2.026997   2.241670  ...   -1.094519   \n",
              "1       2.024433   0.102952  -1.634336   3.621519  ...    1.806486   \n",
              "2       2.769536   0.734942   1.681471   3.229447  ...    1.759080   \n",
              "3       2.684752   0.168206  -1.072321   4.971480  ...    2.171847   \n",
              "4       2.355863   0.318961  -0.570734   4.056542  ...   -0.373413   \n",
              "...          ...        ...        ...        ...  ...         ...   \n",
              "11950   1.352361  -0.396846  -0.353791   3.740440  ...    0.333993   \n",
              "11951   2.003634  -0.300293  -0.423918   3.141109  ...    1.468012   \n",
              "11952   3.165674  -1.520392  -0.195639   3.962691  ...    1.096878   \n",
              "11953   2.382992  -1.060896   0.930493   3.623143  ...    1.006593   \n",
              "11954   1.361515  -1.007086   0.280350   1.368813  ...    1.190370   \n",
              "\n",
              "       feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  \\\n",
              "0       -1.217407   -4.280456    1.512240   -2.306445    2.066388    0.844927   \n",
              "1       -3.477517   -2.064966    1.499805    1.284697    0.189269   -1.563224   \n",
              "2       -2.038839   -2.067219    2.141083    0.055355    0.084739   -1.009925   \n",
              "3       -0.925040   -1.484278    0.666036    0.911519    0.616167    0.092304   \n",
              "4       -0.015773   -2.961445    1.301413    1.375090   -0.107355    0.924390   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "11950   -0.533786   -1.779781    0.228962    0.403641   -0.906588   -0.788647   \n",
              "11951   -1.345186   -1.067598    1.685431    0.879146    0.206344   -1.623952   \n",
              "11952   -0.558995   -3.029989    1.352324    0.351689   -0.326579   -0.382071   \n",
              "11953   -2.548628   -2.803372    1.091970    0.436466    0.235511   -1.625251   \n",
              "11954   -2.805852   -4.221541    2.898248   -0.898532    1.116955   -1.873156   \n",
              "\n",
              "       feature_47  feature_48  feature_49  \n",
              "0       -1.026193          18          58  \n",
              "1       -1.901654           3          35  \n",
              "2       -2.058473           7          50  \n",
              "3       -1.874706          22          47  \n",
              "4       -1.606419          29          52  \n",
              "...           ...         ...         ...  \n",
              "11950   -0.627803          18          45  \n",
              "11951   -2.496700          20          44  \n",
              "11952   -2.995726          17          34  \n",
              "11953   -2.354214          26          58  \n",
              "11954   -2.604914          13          66  \n",
              "\n",
              "[11955 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4421bda-e567-418e-a338-658a7f31858d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>month</th>\n",
              "      <th>n_seconds_1</th>\n",
              "      <th>n_seconds_2</th>\n",
              "      <th>n_seconds_3</th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_40</th>\n",
              "      <th>feature_41</th>\n",
              "      <th>feature_42</th>\n",
              "      <th>feature_43</th>\n",
              "      <th>feature_44</th>\n",
              "      <th>feature_45</th>\n",
              "      <th>feature_46</th>\n",
              "      <th>feature_47</th>\n",
              "      <th>feature_48</th>\n",
              "      <th>feature_49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2155</td>\n",
              "      <td>1</td>\n",
              "      <td>6893.544</td>\n",
              "      <td>246.854</td>\n",
              "      <td>242.636</td>\n",
              "      <td>-1.723524</td>\n",
              "      <td>3.216489</td>\n",
              "      <td>-1.138474</td>\n",
              "      <td>2.026997</td>\n",
              "      <td>2.241670</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.094519</td>\n",
              "      <td>-1.217407</td>\n",
              "      <td>-4.280456</td>\n",
              "      <td>1.512240</td>\n",
              "      <td>-2.306445</td>\n",
              "      <td>2.066388</td>\n",
              "      <td>0.844927</td>\n",
              "      <td>-1.026193</td>\n",
              "      <td>18</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9211</td>\n",
              "      <td>1</td>\n",
              "      <td>4481.065</td>\n",
              "      <td>740.209</td>\n",
              "      <td>263.860</td>\n",
              "      <td>-0.417275</td>\n",
              "      <td>2.024433</td>\n",
              "      <td>0.102952</td>\n",
              "      <td>-1.634336</td>\n",
              "      <td>3.621519</td>\n",
              "      <td>...</td>\n",
              "      <td>1.806486</td>\n",
              "      <td>-3.477517</td>\n",
              "      <td>-2.064966</td>\n",
              "      <td>1.499805</td>\n",
              "      <td>1.284697</td>\n",
              "      <td>0.189269</td>\n",
              "      <td>-1.563224</td>\n",
              "      <td>-1.901654</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3609</td>\n",
              "      <td>1</td>\n",
              "      <td>4340.702</td>\n",
              "      <td>2742.163</td>\n",
              "      <td>318.700</td>\n",
              "      <td>-2.943294</td>\n",
              "      <td>2.769536</td>\n",
              "      <td>0.734942</td>\n",
              "      <td>1.681471</td>\n",
              "      <td>3.229447</td>\n",
              "      <td>...</td>\n",
              "      <td>1.759080</td>\n",
              "      <td>-2.038839</td>\n",
              "      <td>-2.067219</td>\n",
              "      <td>2.141083</td>\n",
              "      <td>0.055355</td>\n",
              "      <td>0.084739</td>\n",
              "      <td>-1.009925</td>\n",
              "      <td>-2.058473</td>\n",
              "      <td>7</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2440</td>\n",
              "      <td>1</td>\n",
              "      <td>4129.666</td>\n",
              "      <td>181.397</td>\n",
              "      <td>155.423</td>\n",
              "      <td>-2.346902</td>\n",
              "      <td>2.684752</td>\n",
              "      <td>0.168206</td>\n",
              "      <td>-1.072321</td>\n",
              "      <td>4.971480</td>\n",
              "      <td>...</td>\n",
              "      <td>2.171847</td>\n",
              "      <td>-0.925040</td>\n",
              "      <td>-1.484278</td>\n",
              "      <td>0.666036</td>\n",
              "      <td>0.911519</td>\n",
              "      <td>0.616167</td>\n",
              "      <td>0.092304</td>\n",
              "      <td>-1.874706</td>\n",
              "      <td>22</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>91</td>\n",
              "      <td>1</td>\n",
              "      <td>3903.944</td>\n",
              "      <td>126.133</td>\n",
              "      <td>100.060</td>\n",
              "      <td>-1.745354</td>\n",
              "      <td>2.355863</td>\n",
              "      <td>0.318961</td>\n",
              "      <td>-0.570734</td>\n",
              "      <td>4.056542</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.373413</td>\n",
              "      <td>-0.015773</td>\n",
              "      <td>-2.961445</td>\n",
              "      <td>1.301413</td>\n",
              "      <td>1.375090</td>\n",
              "      <td>-0.107355</td>\n",
              "      <td>0.924390</td>\n",
              "      <td>-1.606419</td>\n",
              "      <td>29</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11950</th>\n",
              "      <td>5559</td>\n",
              "      <td>1</td>\n",
              "      <td>47.726</td>\n",
              "      <td>40.879</td>\n",
              "      <td>40.647</td>\n",
              "      <td>-2.952961</td>\n",
              "      <td>1.352361</td>\n",
              "      <td>-0.396846</td>\n",
              "      <td>-0.353791</td>\n",
              "      <td>3.740440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333993</td>\n",
              "      <td>-0.533786</td>\n",
              "      <td>-1.779781</td>\n",
              "      <td>0.228962</td>\n",
              "      <td>0.403641</td>\n",
              "      <td>-0.906588</td>\n",
              "      <td>-0.788647</td>\n",
              "      <td>-0.627803</td>\n",
              "      <td>18</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>4508</td>\n",
              "      <td>1</td>\n",
              "      <td>47.471</td>\n",
              "      <td>44.745</td>\n",
              "      <td>41.040</td>\n",
              "      <td>-1.945921</td>\n",
              "      <td>2.003634</td>\n",
              "      <td>-0.300293</td>\n",
              "      <td>-0.423918</td>\n",
              "      <td>3.141109</td>\n",
              "      <td>...</td>\n",
              "      <td>1.468012</td>\n",
              "      <td>-1.345186</td>\n",
              "      <td>-1.067598</td>\n",
              "      <td>1.685431</td>\n",
              "      <td>0.879146</td>\n",
              "      <td>0.206344</td>\n",
              "      <td>-1.623952</td>\n",
              "      <td>-2.496700</td>\n",
              "      <td>20</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11952</th>\n",
              "      <td>8094</td>\n",
              "      <td>1</td>\n",
              "      <td>47.403</td>\n",
              "      <td>45.832</td>\n",
              "      <td>43.932</td>\n",
              "      <td>-1.596234</td>\n",
              "      <td>3.165674</td>\n",
              "      <td>-1.520392</td>\n",
              "      <td>-0.195639</td>\n",
              "      <td>3.962691</td>\n",
              "      <td>...</td>\n",
              "      <td>1.096878</td>\n",
              "      <td>-0.558995</td>\n",
              "      <td>-3.029989</td>\n",
              "      <td>1.352324</td>\n",
              "      <td>0.351689</td>\n",
              "      <td>-0.326579</td>\n",
              "      <td>-0.382071</td>\n",
              "      <td>-2.995726</td>\n",
              "      <td>17</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11953</th>\n",
              "      <td>5891</td>\n",
              "      <td>1</td>\n",
              "      <td>46.044</td>\n",
              "      <td>43.592</td>\n",
              "      <td>40.735</td>\n",
              "      <td>-1.984582</td>\n",
              "      <td>2.382992</td>\n",
              "      <td>-1.060896</td>\n",
              "      <td>0.930493</td>\n",
              "      <td>3.623143</td>\n",
              "      <td>...</td>\n",
              "      <td>1.006593</td>\n",
              "      <td>-2.548628</td>\n",
              "      <td>-2.803372</td>\n",
              "      <td>1.091970</td>\n",
              "      <td>0.436466</td>\n",
              "      <td>0.235511</td>\n",
              "      <td>-1.625251</td>\n",
              "      <td>-2.354214</td>\n",
              "      <td>26</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11954</th>\n",
              "      <td>4123</td>\n",
              "      <td>1</td>\n",
              "      <td>45.802</td>\n",
              "      <td>45.056</td>\n",
              "      <td>40.199</td>\n",
              "      <td>-0.849872</td>\n",
              "      <td>1.361515</td>\n",
              "      <td>-1.007086</td>\n",
              "      <td>0.280350</td>\n",
              "      <td>1.368813</td>\n",
              "      <td>...</td>\n",
              "      <td>1.190370</td>\n",
              "      <td>-2.805852</td>\n",
              "      <td>-4.221541</td>\n",
              "      <td>2.898248</td>\n",
              "      <td>-0.898532</td>\n",
              "      <td>1.116955</td>\n",
              "      <td>-1.873156</td>\n",
              "      <td>-2.604914</td>\n",
              "      <td>13</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11955 rows × 55 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4421bda-e567-418e-a338-658a7f31858d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4421bda-e567-418e-a338-658a7f31858d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4421bda-e567-418e-a338-658a7f31858d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a30a21b7-5c37-4e13-8bd7-a4e1919b4269\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a30a21b7-5c37-4e13-8bd7-a4e1919b4269')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a30a21b7-5c37-4e13-8bd7-a4e1919b4269 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"target1\", \"target2\", \"target3\"], axis=1)\n",
        "y1 = df[[\"target1\"]]\n",
        "y2 = df[[\"target2\"]]\n",
        "y3 = df[[\"target3\"]]\n",
        "\n",
        "\n",
        "# Veriyi eğitim ve test setlerine bölelim\n",
        "X_train, X_test, y1_train, y1_test, y2_train, y2_test, y3_train, y3_test = train_test_split(\n",
        "    X, y1, y2, y3, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Modeli oluşturma\n",
        "input_layer = Input(shape=(55,))\n",
        "embedding_layer = Embedding(input_dim=5000, output_dim=64)(input_layer)\n",
        "lstm_layer = LSTM(64)(embedding_layer)\n",
        "\n",
        "# İlk çıkış\n",
        "output1 = Dense(9, activation='softmax', name='output1')(lstm_layer)\n",
        "\n",
        "# İkinci çıkış\n",
        "output2 = Dense(9, activation='softmax', name='output2')(lstm_layer)\n",
        "\n",
        "# Üçüncü çıkış\n",
        "output3 = Dense(9, activation='softmax', name='output3')(lstm_layer)\n",
        "\n",
        "# Modeli oluştur\n",
        "model = Model(inputs=input_layer, outputs=[output1, output2, output3])\n",
        "\n",
        "# Modeli derle\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'output1': 'sparse_categorical_crossentropy',\n",
        "                    'output2': 'sparse_categorical_crossentropy',\n",
        "                    'output3': 'sparse_categorical_crossentropy'},\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Modeli eğit\n",
        "model.fit(X_train, {'output1': y1_train, 'output2': y2_train, 'output3': y3_train},\n",
        "          validation_data=(X_test, {'output1': y1_test, 'output2': y2_test, 'output3': y3_test}),\n",
        "          epochs=10, batch_size=32)\n",
        "\n",
        "# Modeli değerlendir\n",
        "loss_and_metrics = model.evaluate(X_test, [y1_test, y2_test, y3_test])\n",
        "loss1 = loss_and_metrics[0]\n",
        "accuracy1 = loss_and_metrics[1]\n",
        "loss2 = loss_and_metrics[2]\n",
        "accuracy2 = loss_and_metrics[3]\n",
        "loss3 = loss_and_metrics[4]\n",
        "accuracy3 = loss_and_metrics[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_TMwZ3BzXu4",
        "outputId": "2c8e77d3-e409-429a-8f93-b6c48900e2f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2352/2352 [==============================] - 29s 11ms/step - loss: 4.1429 - output1_loss: 1.0879 - output2_loss: 1.3139 - output3_loss: 1.7411 - output1_accuracy: 0.6326 - output2_accuracy: 0.6135 - output3_accuracy: 0.2754 - val_loss: 4.1064 - val_output1_loss: 1.0813 - val_output2_loss: 1.2960 - val_output3_loss: 1.7292 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.2768\n",
            "Epoch 2/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 4.1101 - output1_loss: 1.0765 - output2_loss: 1.3047 - output3_loss: 1.7290 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.2784 - val_loss: 4.1092 - val_output1_loss: 1.0832 - val_output2_loss: 1.2974 - val_output3_loss: 1.7286 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.2806\n",
            "Epoch 3/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 4.1047 - output1_loss: 1.0757 - output2_loss: 1.3029 - output3_loss: 1.7261 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.2805 - val_loss: 4.0978 - val_output1_loss: 1.0810 - val_output2_loss: 1.2929 - val_output3_loss: 1.7240 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.2795\n",
            "Epoch 4/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 4.0796 - output1_loss: 1.0728 - output2_loss: 1.2960 - output3_loss: 1.7108 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.2920 - val_loss: 4.0568 - val_output1_loss: 1.0753 - val_output2_loss: 1.2835 - val_output3_loss: 1.6979 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.3060\n",
            "Epoch 5/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 4.0393 - output1_loss: 1.0685 - output2_loss: 1.2853 - output3_loss: 1.6855 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.3175 - val_loss: 4.0396 - val_output1_loss: 1.0746 - val_output2_loss: 1.2786 - val_output3_loss: 1.6864 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.3131\n",
            "Epoch 6/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 4.0103 - output1_loss: 1.0659 - output2_loss: 1.2777 - output3_loss: 1.6667 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.3334 - val_loss: 4.0247 - val_output1_loss: 1.0743 - val_output2_loss: 1.2772 - val_output3_loss: 1.6731 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.3293\n",
            "Epoch 7/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.9749 - output1_loss: 1.0623 - output2_loss: 1.2713 - output3_loss: 1.6413 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.3508 - val_loss: 4.0036 - val_output1_loss: 1.0729 - val_output2_loss: 1.2749 - val_output3_loss: 1.6558 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.3453\n",
            "Epoch 8/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.9460 - output1_loss: 1.0596 - output2_loss: 1.2657 - output3_loss: 1.6207 - output1_accuracy: 0.6330 - output2_accuracy: 0.6138 - output3_accuracy: 0.3666 - val_loss: 3.9922 - val_output1_loss: 1.0718 - val_output2_loss: 1.2693 - val_output3_loss: 1.6511 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6194 - val_output3_accuracy: 0.3474\n",
            "Epoch 9/100\n",
            "2352/2352 [==============================] - 19s 8ms/step - loss: 3.9198 - output1_loss: 1.0546 - output2_loss: 1.2606 - output3_loss: 1.6046 - output1_accuracy: 0.6330 - output2_accuracy: 0.6139 - output3_accuracy: 0.3757 - val_loss: 3.9811 - val_output1_loss: 1.0706 - val_output2_loss: 1.2711 - val_output3_loss: 1.6394 - val_output1_accuracy: 0.6279 - val_output2_accuracy: 0.6193 - val_output3_accuracy: 0.3593\n",
            "Epoch 10/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.8974 - output1_loss: 1.0507 - output2_loss: 1.2555 - output3_loss: 1.5912 - output1_accuracy: 0.6332 - output2_accuracy: 0.6142 - output3_accuracy: 0.3851 - val_loss: 3.9838 - val_output1_loss: 1.0695 - val_output2_loss: 1.2723 - val_output3_loss: 1.6419 - val_output1_accuracy: 0.6278 - val_output2_accuracy: 0.6192 - val_output3_accuracy: 0.3572\n",
            "Epoch 11/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.8747 - output1_loss: 1.0454 - output2_loss: 1.2491 - output3_loss: 1.5803 - output1_accuracy: 0.6334 - output2_accuracy: 0.6151 - output3_accuracy: 0.3911 - val_loss: 4.0156 - val_output1_loss: 1.0758 - val_output2_loss: 1.2790 - val_output3_loss: 1.6608 - val_output1_accuracy: 0.6275 - val_output2_accuracy: 0.6178 - val_output3_accuracy: 0.3525\n",
            "Epoch 12/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.8516 - output1_loss: 1.0392 - output2_loss: 1.2428 - output3_loss: 1.5695 - output1_accuracy: 0.6339 - output2_accuracy: 0.6164 - output3_accuracy: 0.3964 - val_loss: 4.0126 - val_output1_loss: 1.0748 - val_output2_loss: 1.2785 - val_output3_loss: 1.6594 - val_output1_accuracy: 0.6268 - val_output2_accuracy: 0.6178 - val_output3_accuracy: 0.3606\n",
            "Epoch 13/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.8320 - output1_loss: 1.0340 - output2_loss: 1.2365 - output3_loss: 1.5615 - output1_accuracy: 0.6344 - output2_accuracy: 0.6170 - output3_accuracy: 0.4009 - val_loss: 4.0025 - val_output1_loss: 1.0772 - val_output2_loss: 1.2782 - val_output3_loss: 1.6470 - val_output1_accuracy: 0.6245 - val_output2_accuracy: 0.6177 - val_output3_accuracy: 0.3584\n",
            "Epoch 14/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.8113 - output1_loss: 1.0281 - output2_loss: 1.2302 - output3_loss: 1.5531 - output1_accuracy: 0.6366 - output2_accuracy: 0.6176 - output3_accuracy: 0.4047 - val_loss: 4.0425 - val_output1_loss: 1.0841 - val_output2_loss: 1.2894 - val_output3_loss: 1.6689 - val_output1_accuracy: 0.6238 - val_output2_accuracy: 0.6154 - val_output3_accuracy: 0.3610\n",
            "Epoch 15/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.7906 - output1_loss: 1.0227 - output2_loss: 1.2234 - output3_loss: 1.5445 - output1_accuracy: 0.6383 - output2_accuracy: 0.6192 - output3_accuracy: 0.4084 - val_loss: 4.0742 - val_output1_loss: 1.0944 - val_output2_loss: 1.3051 - val_output3_loss: 1.6747 - val_output1_accuracy: 0.6186 - val_output2_accuracy: 0.6107 - val_output3_accuracy: 0.3570\n",
            "Epoch 16/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.7695 - output1_loss: 1.0157 - output2_loss: 1.2168 - output3_loss: 1.5369 - output1_accuracy: 0.6418 - output2_accuracy: 0.6221 - output3_accuracy: 0.4122 - val_loss: 4.0356 - val_output1_loss: 1.0873 - val_output2_loss: 1.2903 - val_output3_loss: 1.6581 - val_output1_accuracy: 0.6215 - val_output2_accuracy: 0.6142 - val_output3_accuracy: 0.3596\n",
            "Epoch 17/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.7492 - output1_loss: 1.0096 - output2_loss: 1.2103 - output3_loss: 1.5293 - output1_accuracy: 0.6440 - output2_accuracy: 0.6240 - output3_accuracy: 0.4162 - val_loss: 4.0796 - val_output1_loss: 1.1001 - val_output2_loss: 1.3030 - val_output3_loss: 1.6765 - val_output1_accuracy: 0.6183 - val_output2_accuracy: 0.6076 - val_output3_accuracy: 0.3575\n",
            "Epoch 18/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.7291 - output1_loss: 1.0032 - output2_loss: 1.2030 - output3_loss: 1.5229 - output1_accuracy: 0.6455 - output2_accuracy: 0.6263 - output3_accuracy: 0.4190 - val_loss: 4.0932 - val_output1_loss: 1.0999 - val_output2_loss: 1.3070 - val_output3_loss: 1.6863 - val_output1_accuracy: 0.6181 - val_output2_accuracy: 0.6117 - val_output3_accuracy: 0.3574\n",
            "Epoch 19/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.7115 - output1_loss: 0.9977 - output2_loss: 1.1978 - output3_loss: 1.5160 - output1_accuracy: 0.6479 - output2_accuracy: 0.6284 - output3_accuracy: 0.4223 - val_loss: 4.1349 - val_output1_loss: 1.1167 - val_output2_loss: 1.3216 - val_output3_loss: 1.6966 - val_output1_accuracy: 0.6106 - val_output2_accuracy: 0.6007 - val_output3_accuracy: 0.3529\n",
            "Epoch 20/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.6889 - output1_loss: 0.9908 - output2_loss: 1.1907 - output3_loss: 1.5075 - output1_accuracy: 0.6508 - output2_accuracy: 0.6301 - output3_accuracy: 0.4265 - val_loss: 4.1445 - val_output1_loss: 1.1166 - val_output2_loss: 1.3283 - val_output3_loss: 1.6996 - val_output1_accuracy: 0.6135 - val_output2_accuracy: 0.6012 - val_output3_accuracy: 0.3526\n",
            "Epoch 21/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.6694 - output1_loss: 0.9844 - output2_loss: 1.1842 - output3_loss: 1.5009 - output1_accuracy: 0.6535 - output2_accuracy: 0.6323 - output3_accuracy: 0.4297 - val_loss: 4.1731 - val_output1_loss: 1.1245 - val_output2_loss: 1.3352 - val_output3_loss: 1.7134 - val_output1_accuracy: 0.6115 - val_output2_accuracy: 0.5976 - val_output3_accuracy: 0.3449\n",
            "Epoch 22/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.6497 - output1_loss: 0.9788 - output2_loss: 1.1774 - output3_loss: 1.4934 - output1_accuracy: 0.6547 - output2_accuracy: 0.6337 - output3_accuracy: 0.4306 - val_loss: 4.1718 - val_output1_loss: 1.1266 - val_output2_loss: 1.3371 - val_output3_loss: 1.7081 - val_output1_accuracy: 0.6086 - val_output2_accuracy: 0.5983 - val_output3_accuracy: 0.3526\n",
            "Epoch 23/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.6284 - output1_loss: 0.9719 - output2_loss: 1.1712 - output3_loss: 1.4853 - output1_accuracy: 0.6575 - output2_accuracy: 0.6348 - output3_accuracy: 0.4343 - val_loss: 4.2147 - val_output1_loss: 1.1440 - val_output2_loss: 1.3525 - val_output3_loss: 1.7182 - val_output1_accuracy: 0.6108 - val_output2_accuracy: 0.6010 - val_output3_accuracy: 0.3480\n",
            "Epoch 24/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.6088 - output1_loss: 0.9657 - output2_loss: 1.1645 - output3_loss: 1.4786 - output1_accuracy: 0.6589 - output2_accuracy: 0.6373 - output3_accuracy: 0.4379 - val_loss: 4.2497 - val_output1_loss: 1.1461 - val_output2_loss: 1.3615 - val_output3_loss: 1.7421 - val_output1_accuracy: 0.6103 - val_output2_accuracy: 0.5986 - val_output3_accuracy: 0.3501\n",
            "Epoch 25/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.5898 - output1_loss: 0.9600 - output2_loss: 1.1587 - output3_loss: 1.4711 - output1_accuracy: 0.6610 - output2_accuracy: 0.6389 - output3_accuracy: 0.4438 - val_loss: 4.2193 - val_output1_loss: 1.1423 - val_output2_loss: 1.3550 - val_output3_loss: 1.7220 - val_output1_accuracy: 0.6094 - val_output2_accuracy: 0.5995 - val_output3_accuracy: 0.3456\n",
            "Epoch 26/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.5687 - output1_loss: 0.9542 - output2_loss: 1.1522 - output3_loss: 1.4624 - output1_accuracy: 0.6643 - output2_accuracy: 0.6401 - output3_accuracy: 0.4471 - val_loss: 4.2958 - val_output1_loss: 1.1635 - val_output2_loss: 1.3782 - val_output3_loss: 1.7541 - val_output1_accuracy: 0.6051 - val_output2_accuracy: 0.5958 - val_output3_accuracy: 0.3468\n",
            "Epoch 27/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.5489 - output1_loss: 0.9472 - output2_loss: 1.1447 - output3_loss: 1.4569 - output1_accuracy: 0.6648 - output2_accuracy: 0.6424 - output3_accuracy: 0.4472 - val_loss: 4.2803 - val_output1_loss: 1.1562 - val_output2_loss: 1.3846 - val_output3_loss: 1.7396 - val_output1_accuracy: 0.6001 - val_output2_accuracy: 0.5869 - val_output3_accuracy: 0.3452\n",
            "Epoch 28/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.5295 - output1_loss: 0.9404 - output2_loss: 1.1392 - output3_loss: 1.4499 - output1_accuracy: 0.6682 - output2_accuracy: 0.6444 - output3_accuracy: 0.4522 - val_loss: 4.3398 - val_output1_loss: 1.1810 - val_output2_loss: 1.3946 - val_output3_loss: 1.7641 - val_output1_accuracy: 0.6054 - val_output2_accuracy: 0.5945 - val_output3_accuracy: 0.3453\n",
            "Epoch 29/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.5097 - output1_loss: 0.9341 - output2_loss: 1.1339 - output3_loss: 1.4416 - output1_accuracy: 0.6703 - output2_accuracy: 0.6460 - output3_accuracy: 0.4544 - val_loss: 4.3998 - val_output1_loss: 1.1954 - val_output2_loss: 1.4074 - val_output3_loss: 1.7970 - val_output1_accuracy: 0.6048 - val_output2_accuracy: 0.5921 - val_output3_accuracy: 0.3403\n",
            "Epoch 30/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.4850 - output1_loss: 0.9264 - output2_loss: 1.1256 - output3_loss: 1.4330 - output1_accuracy: 0.6728 - output2_accuracy: 0.6482 - output3_accuracy: 0.4594 - val_loss: 4.3995 - val_output1_loss: 1.1992 - val_output2_loss: 1.4269 - val_output3_loss: 1.7733 - val_output1_accuracy: 0.5965 - val_output2_accuracy: 0.5859 - val_output3_accuracy: 0.3457\n",
            "Epoch 31/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.4702 - output1_loss: 0.9218 - output2_loss: 1.1201 - output3_loss: 1.4283 - output1_accuracy: 0.6743 - output2_accuracy: 0.6497 - output3_accuracy: 0.4615 - val_loss: 4.4271 - val_output1_loss: 1.2039 - val_output2_loss: 1.4313 - val_output3_loss: 1.7919 - val_output1_accuracy: 0.5981 - val_output2_accuracy: 0.5851 - val_output3_accuracy: 0.3444\n",
            "Epoch 32/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.4517 - output1_loss: 0.9155 - output2_loss: 1.1143 - output3_loss: 1.4219 - output1_accuracy: 0.6764 - output2_accuracy: 0.6511 - output3_accuracy: 0.4626 - val_loss: 4.5191 - val_output1_loss: 1.2420 - val_output2_loss: 1.4812 - val_output3_loss: 1.7959 - val_output1_accuracy: 0.5805 - val_output2_accuracy: 0.5669 - val_output3_accuracy: 0.3437\n",
            "Epoch 33/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.4305 - output1_loss: 0.9087 - output2_loss: 1.1065 - output3_loss: 1.4153 - output1_accuracy: 0.6797 - output2_accuracy: 0.6537 - output3_accuracy: 0.4680 - val_loss: 4.4460 - val_output1_loss: 1.2059 - val_output2_loss: 1.4521 - val_output3_loss: 1.7879 - val_output1_accuracy: 0.5903 - val_output2_accuracy: 0.5775 - val_output3_accuracy: 0.3447\n",
            "Epoch 34/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.4125 - output1_loss: 0.9025 - output2_loss: 1.1010 - output3_loss: 1.4090 - output1_accuracy: 0.6815 - output2_accuracy: 0.6541 - output3_accuracy: 0.4706 - val_loss: 4.5266 - val_output1_loss: 1.2410 - val_output2_loss: 1.4660 - val_output3_loss: 1.8197 - val_output1_accuracy: 0.5982 - val_output2_accuracy: 0.5883 - val_output3_accuracy: 0.3433\n",
            "Epoch 35/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.3965 - output1_loss: 0.8988 - output2_loss: 1.0947 - output3_loss: 1.4030 - output1_accuracy: 0.6831 - output2_accuracy: 0.6572 - output3_accuracy: 0.4732 - val_loss: 4.5208 - val_output1_loss: 1.2312 - val_output2_loss: 1.4776 - val_output3_loss: 1.8120 - val_output1_accuracy: 0.5881 - val_output2_accuracy: 0.5792 - val_output3_accuracy: 0.3419\n",
            "Epoch 36/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.3741 - output1_loss: 0.8917 - output2_loss: 1.0880 - output3_loss: 1.3944 - output1_accuracy: 0.6847 - output2_accuracy: 0.6591 - output3_accuracy: 0.4755 - val_loss: 4.5716 - val_output1_loss: 1.2488 - val_output2_loss: 1.4851 - val_output3_loss: 1.8377 - val_output1_accuracy: 0.5898 - val_output2_accuracy: 0.5800 - val_output3_accuracy: 0.3438\n",
            "Epoch 37/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.3604 - output1_loss: 0.8880 - output2_loss: 1.0836 - output3_loss: 1.3889 - output1_accuracy: 0.6864 - output2_accuracy: 0.6603 - output3_accuracy: 0.4780 - val_loss: 4.5434 - val_output1_loss: 1.2425 - val_output2_loss: 1.4787 - val_output3_loss: 1.8223 - val_output1_accuracy: 0.5890 - val_output2_accuracy: 0.5742 - val_output3_accuracy: 0.3386\n",
            "Epoch 38/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.3440 - output1_loss: 0.8808 - output2_loss: 1.0780 - output3_loss: 1.3852 - output1_accuracy: 0.6882 - output2_accuracy: 0.6620 - output3_accuracy: 0.4797 - val_loss: 4.6850 - val_output1_loss: 1.2920 - val_output2_loss: 1.5533 - val_output3_loss: 1.8397 - val_output1_accuracy: 0.5653 - val_output2_accuracy: 0.5520 - val_output3_accuracy: 0.3384\n",
            "Epoch 39/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.3262 - output1_loss: 0.8753 - output2_loss: 1.0729 - output3_loss: 1.3780 - output1_accuracy: 0.6896 - output2_accuracy: 0.6636 - output3_accuracy: 0.4836 - val_loss: 4.6213 - val_output1_loss: 1.2599 - val_output2_loss: 1.5080 - val_output3_loss: 1.8534 - val_output1_accuracy: 0.5927 - val_output2_accuracy: 0.5764 - val_output3_accuracy: 0.3415\n",
            "Epoch 40/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.3096 - output1_loss: 0.8711 - output2_loss: 1.0659 - output3_loss: 1.3726 - output1_accuracy: 0.6927 - output2_accuracy: 0.6662 - output3_accuracy: 0.4848 - val_loss: 4.6905 - val_output1_loss: 1.2814 - val_output2_loss: 1.5389 - val_output3_loss: 1.8703 - val_output1_accuracy: 0.5843 - val_output2_accuracy: 0.5670 - val_output3_accuracy: 0.3338\n",
            "Epoch 41/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.2953 - output1_loss: 0.8667 - output2_loss: 1.0610 - output3_loss: 1.3676 - output1_accuracy: 0.6937 - output2_accuracy: 0.6669 - output3_accuracy: 0.4859 - val_loss: 4.7100 - val_output1_loss: 1.2964 - val_output2_loss: 1.5395 - val_output3_loss: 1.8741 - val_output1_accuracy: 0.5851 - val_output2_accuracy: 0.5692 - val_output3_accuracy: 0.3362\n",
            "Epoch 42/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.2804 - output1_loss: 0.8623 - output2_loss: 1.0572 - output3_loss: 1.3609 - output1_accuracy: 0.6950 - output2_accuracy: 0.6683 - output3_accuracy: 0.4892 - val_loss: 4.7469 - val_output1_loss: 1.3050 - val_output2_loss: 1.5525 - val_output3_loss: 1.8894 - val_output1_accuracy: 0.5734 - val_output2_accuracy: 0.5637 - val_output3_accuracy: 0.3368\n",
            "Epoch 43/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.2628 - output1_loss: 0.8574 - output2_loss: 1.0510 - output3_loss: 1.3545 - output1_accuracy: 0.6970 - output2_accuracy: 0.6701 - output3_accuracy: 0.4903 - val_loss: 4.7710 - val_output1_loss: 1.3131 - val_output2_loss: 1.5644 - val_output3_loss: 1.8935 - val_output1_accuracy: 0.5793 - val_output2_accuracy: 0.5637 - val_output3_accuracy: 0.3376\n",
            "Epoch 44/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.2478 - output1_loss: 0.8511 - output2_loss: 1.0454 - output3_loss: 1.3513 - output1_accuracy: 0.6983 - output2_accuracy: 0.6717 - output3_accuracy: 0.4929 - val_loss: 4.8120 - val_output1_loss: 1.3433 - val_output2_loss: 1.5682 - val_output3_loss: 1.9004 - val_output1_accuracy: 0.5848 - val_output2_accuracy: 0.5730 - val_output3_accuracy: 0.3325\n",
            "Epoch 45/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.2358 - output1_loss: 0.8476 - output2_loss: 1.0411 - output3_loss: 1.3471 - output1_accuracy: 0.6996 - output2_accuracy: 0.6732 - output3_accuracy: 0.4954 - val_loss: 4.7939 - val_output1_loss: 1.3172 - val_output2_loss: 1.5695 - val_output3_loss: 1.9071 - val_output1_accuracy: 0.5796 - val_output2_accuracy: 0.5671 - val_output3_accuracy: 0.3359\n",
            "Epoch 46/100\n",
            "2352/2352 [==============================] - 20s 8ms/step - loss: 3.2237 - output1_loss: 0.8439 - output2_loss: 1.0371 - output3_loss: 1.3427 - output1_accuracy: 0.7011 - output2_accuracy: 0.6737 - output3_accuracy: 0.4969 - val_loss: 4.8602 - val_output1_loss: 1.3502 - val_output2_loss: 1.5941 - val_output3_loss: 1.9160 - val_output1_accuracy: 0.5771 - val_output2_accuracy: 0.5680 - val_output3_accuracy: 0.3335\n",
            "Epoch 47/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.2110 - output1_loss: 0.8415 - output2_loss: 1.0327 - output3_loss: 1.3369 - output1_accuracy: 0.7006 - output2_accuracy: 0.6754 - output3_accuracy: 0.4989 - val_loss: 4.8518 - val_output1_loss: 1.3501 - val_output2_loss: 1.6011 - val_output3_loss: 1.9007 - val_output1_accuracy: 0.5746 - val_output2_accuracy: 0.5608 - val_output3_accuracy: 0.3323\n",
            "Epoch 48/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1962 - output1_loss: 0.8351 - output2_loss: 1.0280 - output3_loss: 1.3330 - output1_accuracy: 0.7028 - output2_accuracy: 0.6775 - output3_accuracy: 0.5008 - val_loss: 4.8503 - val_output1_loss: 1.3451 - val_output2_loss: 1.5945 - val_output3_loss: 1.9107 - val_output1_accuracy: 0.5757 - val_output2_accuracy: 0.5656 - val_output3_accuracy: 0.3296\n",
            "Epoch 49/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1861 - output1_loss: 0.8318 - output2_loss: 1.0234 - output3_loss: 1.3309 - output1_accuracy: 0.7055 - output2_accuracy: 0.6771 - output3_accuracy: 0.5008 - val_loss: 4.8743 - val_output1_loss: 1.3492 - val_output2_loss: 1.5976 - val_output3_loss: 1.9275 - val_output1_accuracy: 0.5826 - val_output2_accuracy: 0.5644 - val_output3_accuracy: 0.3316\n",
            "Epoch 50/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1696 - output1_loss: 0.8273 - output2_loss: 1.0189 - output3_loss: 1.3233 - output1_accuracy: 0.7057 - output2_accuracy: 0.6802 - output3_accuracy: 0.5054 - val_loss: 4.8642 - val_output1_loss: 1.3511 - val_output2_loss: 1.5893 - val_output3_loss: 1.9239 - val_output1_accuracy: 0.5896 - val_output2_accuracy: 0.5785 - val_output3_accuracy: 0.3286\n",
            "Epoch 51/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1611 - output1_loss: 0.8247 - output2_loss: 1.0151 - output3_loss: 1.3214 - output1_accuracy: 0.7073 - output2_accuracy: 0.6808 - output3_accuracy: 0.5042 - val_loss: 5.0020 - val_output1_loss: 1.3966 - val_output2_loss: 1.6520 - val_output3_loss: 1.9535 - val_output1_accuracy: 0.5710 - val_output2_accuracy: 0.5614 - val_output3_accuracy: 0.3315\n",
            "Epoch 52/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.1446 - output1_loss: 0.8205 - output2_loss: 1.0098 - output3_loss: 1.3143 - output1_accuracy: 0.7075 - output2_accuracy: 0.6827 - output3_accuracy: 0.5077 - val_loss: 4.9720 - val_output1_loss: 1.3805 - val_output2_loss: 1.6531 - val_output3_loss: 1.9383 - val_output1_accuracy: 0.5704 - val_output2_accuracy: 0.5565 - val_output3_accuracy: 0.3274\n",
            "Epoch 53/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1308 - output1_loss: 0.8161 - output2_loss: 1.0046 - output3_loss: 1.3102 - output1_accuracy: 0.7115 - output2_accuracy: 0.6837 - output3_accuracy: 0.5090 - val_loss: 4.9848 - val_output1_loss: 1.3906 - val_output2_loss: 1.6319 - val_output3_loss: 1.9623 - val_output1_accuracy: 0.5781 - val_output2_accuracy: 0.5680 - val_output3_accuracy: 0.3267\n",
            "Epoch 54/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 3.1258 - output1_loss: 0.8134 - output2_loss: 1.0037 - output3_loss: 1.3086 - output1_accuracy: 0.7115 - output2_accuracy: 0.6848 - output3_accuracy: 0.5088 - val_loss: 5.0642 - val_output1_loss: 1.4086 - val_output2_loss: 1.6708 - val_output3_loss: 1.9848 - val_output1_accuracy: 0.5708 - val_output2_accuracy: 0.5623 - val_output3_accuracy: 0.3281\n",
            "Epoch 55/100\n",
            "2352/2352 [==============================] - 20s 9ms/step - loss: 3.1120 - output1_loss: 0.8104 - output2_loss: 0.9987 - output3_loss: 1.3028 - output1_accuracy: 0.7125 - output2_accuracy: 0.6861 - output3_accuracy: 0.5123 - val_loss: 4.9967 - val_output1_loss: 1.3883 - val_output2_loss: 1.6570 - val_output3_loss: 1.9514 - val_output1_accuracy: 0.5755 - val_output2_accuracy: 0.5644 - val_output3_accuracy: 0.3299\n",
            "Epoch 56/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.1022 - output1_loss: 0.8065 - output2_loss: 0.9962 - output3_loss: 1.2995 - output1_accuracy: 0.7127 - output2_accuracy: 0.6863 - output3_accuracy: 0.5134 - val_loss: 5.0879 - val_output1_loss: 1.4195 - val_output2_loss: 1.6977 - val_output3_loss: 1.9708 - val_output1_accuracy: 0.5559 - val_output2_accuracy: 0.5347 - val_output3_accuracy: 0.3238\n",
            "Epoch 57/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0899 - output1_loss: 0.8031 - output2_loss: 0.9906 - output3_loss: 1.2961 - output1_accuracy: 0.7139 - output2_accuracy: 0.6876 - output3_accuracy: 0.5153 - val_loss: 5.0844 - val_output1_loss: 1.4188 - val_output2_loss: 1.6872 - val_output3_loss: 1.9784 - val_output1_accuracy: 0.5744 - val_output2_accuracy: 0.5609 - val_output3_accuracy: 0.3250\n",
            "Epoch 58/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0828 - output1_loss: 0.7997 - output2_loss: 0.9885 - output3_loss: 1.2946 - output1_accuracy: 0.7157 - output2_accuracy: 0.6883 - output3_accuracy: 0.5154 - val_loss: 5.1251 - val_output1_loss: 1.4354 - val_output2_loss: 1.7058 - val_output3_loss: 1.9839 - val_output1_accuracy: 0.5637 - val_output2_accuracy: 0.5472 - val_output3_accuracy: 0.3221\n",
            "Epoch 59/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0682 - output1_loss: 0.7961 - output2_loss: 0.9851 - output3_loss: 1.2870 - output1_accuracy: 0.7163 - output2_accuracy: 0.6897 - output3_accuracy: 0.5197 - val_loss: 5.2021 - val_output1_loss: 1.4634 - val_output2_loss: 1.7284 - val_output3_loss: 2.0103 - val_output1_accuracy: 0.5617 - val_output2_accuracy: 0.5485 - val_output3_accuracy: 0.3248\n",
            "Epoch 60/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 3.0580 - output1_loss: 0.7936 - output2_loss: 0.9809 - output3_loss: 1.2835 - output1_accuracy: 0.7170 - output2_accuracy: 0.6908 - output3_accuracy: 0.5204 - val_loss: 5.0795 - val_output1_loss: 1.4181 - val_output2_loss: 1.6835 - val_output3_loss: 1.9780 - val_output1_accuracy: 0.5689 - val_output2_accuracy: 0.5532 - val_output3_accuracy: 0.3228\n",
            "Epoch 61/100\n",
            "2352/2352 [==============================] - 23s 10ms/step - loss: 3.0596 - output1_loss: 0.7943 - output2_loss: 0.9811 - output3_loss: 1.2843 - output1_accuracy: 0.7177 - output2_accuracy: 0.6914 - output3_accuracy: 0.5201 - val_loss: 5.1496 - val_output1_loss: 1.4427 - val_output2_loss: 1.7071 - val_output3_loss: 1.9998 - val_output1_accuracy: 0.5631 - val_output2_accuracy: 0.5523 - val_output3_accuracy: 0.3164\n",
            "Epoch 62/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0380 - output1_loss: 0.7866 - output2_loss: 0.9743 - output3_loss: 1.2770 - output1_accuracy: 0.7191 - output2_accuracy: 0.6917 - output3_accuracy: 0.5209 - val_loss: 5.1440 - val_output1_loss: 1.4434 - val_output2_loss: 1.7021 - val_output3_loss: 1.9986 - val_output1_accuracy: 0.5721 - val_output2_accuracy: 0.5593 - val_output3_accuracy: 0.3237\n",
            "Epoch 63/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0307 - output1_loss: 0.7859 - output2_loss: 0.9707 - output3_loss: 1.2741 - output1_accuracy: 0.7199 - output2_accuracy: 0.6936 - output3_accuracy: 0.5245 - val_loss: 5.2119 - val_output1_loss: 1.4621 - val_output2_loss: 1.7382 - val_output3_loss: 2.0116 - val_output1_accuracy: 0.5624 - val_output2_accuracy: 0.5458 - val_output3_accuracy: 0.3204\n",
            "Epoch 64/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 3.0266 - output1_loss: 0.7825 - output2_loss: 0.9710 - output3_loss: 1.2731 - output1_accuracy: 0.7212 - output2_accuracy: 0.6940 - output3_accuracy: 0.5236 - val_loss: 5.1605 - val_output1_loss: 1.4426 - val_output2_loss: 1.7114 - val_output3_loss: 2.0066 - val_output1_accuracy: 0.5692 - val_output2_accuracy: 0.5548 - val_output3_accuracy: 0.3253\n",
            "Epoch 65/100\n",
            "2352/2352 [==============================] - 23s 10ms/step - loss: 3.0150 - output1_loss: 0.7808 - output2_loss: 0.9657 - output3_loss: 1.2686 - output1_accuracy: 0.7217 - output2_accuracy: 0.6947 - output3_accuracy: 0.5258 - val_loss: 5.2834 - val_output1_loss: 1.4813 - val_output2_loss: 1.7446 - val_output3_loss: 2.0576 - val_output1_accuracy: 0.5676 - val_output2_accuracy: 0.5583 - val_output3_accuracy: 0.3228\n",
            "Epoch 66/100\n",
            "2352/2352 [==============================] - 22s 10ms/step - loss: 3.0067 - output1_loss: 0.7759 - output2_loss: 0.9657 - output3_loss: 1.2652 - output1_accuracy: 0.7231 - output2_accuracy: 0.6960 - output3_accuracy: 0.5268 - val_loss: 5.2925 - val_output1_loss: 1.5028 - val_output2_loss: 1.7612 - val_output3_loss: 2.0285 - val_output1_accuracy: 0.5641 - val_output2_accuracy: 0.5510 - val_output3_accuracy: 0.3225\n",
            "Epoch 67/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.9953 - output1_loss: 0.7736 - output2_loss: 0.9599 - output3_loss: 1.2618 - output1_accuracy: 0.7236 - output2_accuracy: 0.6963 - output3_accuracy: 0.5288 - val_loss: 5.2619 - val_output1_loss: 1.4851 - val_output2_loss: 1.7419 - val_output3_loss: 2.0349 - val_output1_accuracy: 0.5720 - val_output2_accuracy: 0.5540 - val_output3_accuracy: 0.3247\n",
            "Epoch 68/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9860 - output1_loss: 0.7711 - output2_loss: 0.9558 - output3_loss: 1.2591 - output1_accuracy: 0.7250 - output2_accuracy: 0.6990 - output3_accuracy: 0.5304 - val_loss: 5.3497 - val_output1_loss: 1.5053 - val_output2_loss: 1.7893 - val_output3_loss: 2.0551 - val_output1_accuracy: 0.5619 - val_output2_accuracy: 0.5464 - val_output3_accuracy: 0.3191\n",
            "Epoch 69/100\n",
            "2352/2352 [==============================] - 23s 10ms/step - loss: 2.9844 - output1_loss: 0.7713 - output2_loss: 0.9572 - output3_loss: 1.2559 - output1_accuracy: 0.7253 - output2_accuracy: 0.6973 - output3_accuracy: 0.5315 - val_loss: 5.3190 - val_output1_loss: 1.4985 - val_output2_loss: 1.7784 - val_output3_loss: 2.0421 - val_output1_accuracy: 0.5652 - val_output2_accuracy: 0.5530 - val_output3_accuracy: 0.3229\n",
            "Epoch 70/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9831 - output1_loss: 0.7710 - output2_loss: 0.9560 - output3_loss: 1.2561 - output1_accuracy: 0.7254 - output2_accuracy: 0.6982 - output3_accuracy: 0.5299 - val_loss: 5.3502 - val_output1_loss: 1.5032 - val_output2_loss: 1.7764 - val_output3_loss: 2.0707 - val_output1_accuracy: 0.5674 - val_output2_accuracy: 0.5498 - val_output3_accuracy: 0.3192\n",
            "Epoch 71/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9671 - output1_loss: 0.7653 - output2_loss: 0.9495 - output3_loss: 1.2523 - output1_accuracy: 0.7263 - output2_accuracy: 0.7011 - output3_accuracy: 0.5305 - val_loss: 5.3760 - val_output1_loss: 1.5123 - val_output2_loss: 1.8095 - val_output3_loss: 2.0543 - val_output1_accuracy: 0.5587 - val_output2_accuracy: 0.5417 - val_output3_accuracy: 0.3228\n",
            "Epoch 72/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9618 - output1_loss: 0.7653 - output2_loss: 0.9488 - output3_loss: 1.2478 - output1_accuracy: 0.7273 - output2_accuracy: 0.6997 - output3_accuracy: 0.5345 - val_loss: 5.3674 - val_output1_loss: 1.5041 - val_output2_loss: 1.8016 - val_output3_loss: 2.0617 - val_output1_accuracy: 0.5556 - val_output2_accuracy: 0.5428 - val_output3_accuracy: 0.3173\n",
            "Epoch 73/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9543 - output1_loss: 0.7635 - output2_loss: 0.9462 - output3_loss: 1.2446 - output1_accuracy: 0.7277 - output2_accuracy: 0.7010 - output3_accuracy: 0.5351 - val_loss: 5.4134 - val_output1_loss: 1.5189 - val_output2_loss: 1.8121 - val_output3_loss: 2.0824 - val_output1_accuracy: 0.5512 - val_output2_accuracy: 0.5349 - val_output3_accuracy: 0.3183\n",
            "Epoch 74/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9449 - output1_loss: 0.7581 - output2_loss: 0.9443 - output3_loss: 1.2425 - output1_accuracy: 0.7290 - output2_accuracy: 0.7008 - output3_accuracy: 0.5354 - val_loss: 5.3651 - val_output1_loss: 1.5184 - val_output2_loss: 1.8003 - val_output3_loss: 2.0464 - val_output1_accuracy: 0.5517 - val_output2_accuracy: 0.5363 - val_output3_accuracy: 0.3181\n",
            "Epoch 75/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.9449 - output1_loss: 0.7582 - output2_loss: 0.9421 - output3_loss: 1.2446 - output1_accuracy: 0.7294 - output2_accuracy: 0.7017 - output3_accuracy: 0.5355 - val_loss: 5.4547 - val_output1_loss: 1.5453 - val_output2_loss: 1.8318 - val_output3_loss: 2.0776 - val_output1_accuracy: 0.5568 - val_output2_accuracy: 0.5341 - val_output3_accuracy: 0.3156\n",
            "Epoch 76/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.9296 - output1_loss: 0.7555 - output2_loss: 0.9381 - output3_loss: 1.2360 - output1_accuracy: 0.7297 - output2_accuracy: 0.7026 - output3_accuracy: 0.5373 - val_loss: 5.4536 - val_output1_loss: 1.5400 - val_output2_loss: 1.8285 - val_output3_loss: 2.0852 - val_output1_accuracy: 0.5527 - val_output2_accuracy: 0.5358 - val_output3_accuracy: 0.3193\n",
            "Epoch 77/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9330 - output1_loss: 0.7541 - output2_loss: 0.9398 - output3_loss: 1.2391 - output1_accuracy: 0.7305 - output2_accuracy: 0.7021 - output3_accuracy: 0.5367 - val_loss: 5.3718 - val_output1_loss: 1.5160 - val_output2_loss: 1.8085 - val_output3_loss: 2.0473 - val_output1_accuracy: 0.5493 - val_output2_accuracy: 0.5342 - val_output3_accuracy: 0.3218\n",
            "Epoch 78/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9250 - output1_loss: 0.7540 - output2_loss: 0.9353 - output3_loss: 1.2356 - output1_accuracy: 0.7311 - output2_accuracy: 0.7051 - output3_accuracy: 0.5381 - val_loss: 5.4751 - val_output1_loss: 1.5437 - val_output2_loss: 1.8389 - val_output3_loss: 2.0924 - val_output1_accuracy: 0.5590 - val_output2_accuracy: 0.5423 - val_output3_accuracy: 0.3163\n",
            "Epoch 79/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.9374 - output1_loss: 0.7554 - output2_loss: 0.9406 - output3_loss: 1.2414 - output1_accuracy: 0.7311 - output2_accuracy: 0.7024 - output3_accuracy: 0.5364 - val_loss: 5.4301 - val_output1_loss: 1.5329 - val_output2_loss: 1.8071 - val_output3_loss: 2.0901 - val_output1_accuracy: 0.5625 - val_output2_accuracy: 0.5483 - val_output3_accuracy: 0.3179\n",
            "Epoch 80/100\n",
            "2352/2352 [==============================] - 24s 10ms/step - loss: 2.8969 - output1_loss: 0.7454 - output2_loss: 0.9245 - output3_loss: 1.2269 - output1_accuracy: 0.7333 - output2_accuracy: 0.7065 - output3_accuracy: 0.5412 - val_loss: 5.5279 - val_output1_loss: 1.5685 - val_output2_loss: 1.8505 - val_output3_loss: 2.1089 - val_output1_accuracy: 0.5465 - val_output2_accuracy: 0.5286 - val_output3_accuracy: 0.3178\n",
            "Epoch 81/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9045 - output1_loss: 0.7460 - output2_loss: 0.9301 - output3_loss: 1.2284 - output1_accuracy: 0.7338 - output2_accuracy: 0.7056 - output3_accuracy: 0.5404 - val_loss: 5.5316 - val_output1_loss: 1.5803 - val_output2_loss: 1.8465 - val_output3_loss: 2.1049 - val_output1_accuracy: 0.5529 - val_output2_accuracy: 0.5367 - val_output3_accuracy: 0.3167\n",
            "Epoch 82/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.9003 - output1_loss: 0.7453 - output2_loss: 0.9280 - output3_loss: 1.2270 - output1_accuracy: 0.7332 - output2_accuracy: 0.7064 - output3_accuracy: 0.5411 - val_loss: 5.4920 - val_output1_loss: 1.5625 - val_output2_loss: 1.8282 - val_output3_loss: 2.1014 - val_output1_accuracy: 0.5675 - val_output2_accuracy: 0.5495 - val_output3_accuracy: 0.3172\n",
            "Epoch 83/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8900 - output1_loss: 0.7430 - output2_loss: 0.9236 - output3_loss: 1.2235 - output1_accuracy: 0.7350 - output2_accuracy: 0.7074 - output3_accuracy: 0.5417 - val_loss: 5.4831 - val_output1_loss: 1.5462 - val_output2_loss: 1.8412 - val_output3_loss: 2.0956 - val_output1_accuracy: 0.5496 - val_output2_accuracy: 0.5335 - val_output3_accuracy: 0.3112\n",
            "Epoch 84/100\n",
            "2352/2352 [==============================] - 23s 10ms/step - loss: 2.8889 - output1_loss: 0.7435 - output2_loss: 0.9235 - output3_loss: 1.2219 - output1_accuracy: 0.7339 - output2_accuracy: 0.7068 - output3_accuracy: 0.5422 - val_loss: 5.5248 - val_output1_loss: 1.5691 - val_output2_loss: 1.8361 - val_output3_loss: 2.1196 - val_output1_accuracy: 0.5468 - val_output2_accuracy: 0.5348 - val_output3_accuracy: 0.3172\n",
            "Epoch 85/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8846 - output1_loss: 0.7403 - output2_loss: 0.9218 - output3_loss: 1.2224 - output1_accuracy: 0.7347 - output2_accuracy: 0.7075 - output3_accuracy: 0.5428 - val_loss: 5.4631 - val_output1_loss: 1.5458 - val_output2_loss: 1.8345 - val_output3_loss: 2.0829 - val_output1_accuracy: 0.5451 - val_output2_accuracy: 0.5290 - val_output3_accuracy: 0.3167\n",
            "Epoch 86/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8735 - output1_loss: 0.7382 - output2_loss: 0.9178 - output3_loss: 1.2175 - output1_accuracy: 0.7356 - output2_accuracy: 0.7096 - output3_accuracy: 0.5438 - val_loss: 5.5723 - val_output1_loss: 1.5863 - val_output2_loss: 1.8664 - val_output3_loss: 2.1196 - val_output1_accuracy: 0.5606 - val_output2_accuracy: 0.5439 - val_output3_accuracy: 0.3136\n",
            "Epoch 87/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8746 - output1_loss: 0.7371 - output2_loss: 0.9200 - output3_loss: 1.2175 - output1_accuracy: 0.7363 - output2_accuracy: 0.7085 - output3_accuracy: 0.5448 - val_loss: 5.4958 - val_output1_loss: 1.5500 - val_output2_loss: 1.8257 - val_output3_loss: 2.1202 - val_output1_accuracy: 0.5541 - val_output2_accuracy: 0.5409 - val_output3_accuracy: 0.3156\n",
            "Epoch 88/100\n",
            "2352/2352 [==============================] - 24s 10ms/step - loss: 2.8687 - output1_loss: 0.7352 - output2_loss: 0.9171 - output3_loss: 1.2163 - output1_accuracy: 0.7362 - output2_accuracy: 0.7085 - output3_accuracy: 0.5454 - val_loss: 5.5606 - val_output1_loss: 1.5751 - val_output2_loss: 1.8521 - val_output3_loss: 2.1334 - val_output1_accuracy: 0.5572 - val_output2_accuracy: 0.5434 - val_output3_accuracy: 0.3158\n",
            "Epoch 89/100\n",
            "2352/2352 [==============================] - 24s 10ms/step - loss: 2.8546 - output1_loss: 0.7320 - output2_loss: 0.9109 - output3_loss: 1.2117 - output1_accuracy: 0.7378 - output2_accuracy: 0.7095 - output3_accuracy: 0.5491 - val_loss: 5.5665 - val_output1_loss: 1.5832 - val_output2_loss: 1.8652 - val_output3_loss: 2.1181 - val_output1_accuracy: 0.5576 - val_output2_accuracy: 0.5479 - val_output3_accuracy: 0.3171\n",
            "Epoch 90/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8608 - output1_loss: 0.7339 - output2_loss: 0.9149 - output3_loss: 1.2120 - output1_accuracy: 0.7374 - output2_accuracy: 0.7077 - output3_accuracy: 0.5471 - val_loss: 5.5338 - val_output1_loss: 1.5708 - val_output2_loss: 1.8367 - val_output3_loss: 2.1262 - val_output1_accuracy: 0.5570 - val_output2_accuracy: 0.5367 - val_output3_accuracy: 0.3130\n",
            "Epoch 91/100\n",
            "2352/2352 [==============================] - 23s 10ms/step - loss: 2.8542 - output1_loss: 0.7313 - output2_loss: 0.9138 - output3_loss: 1.2090 - output1_accuracy: 0.7377 - output2_accuracy: 0.7105 - output3_accuracy: 0.5496 - val_loss: 5.5575 - val_output1_loss: 1.5829 - val_output2_loss: 1.8429 - val_output3_loss: 2.1317 - val_output1_accuracy: 0.5587 - val_output2_accuracy: 0.5391 - val_output3_accuracy: 0.3127\n",
            "Epoch 92/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8438 - output1_loss: 0.7286 - output2_loss: 0.9088 - output3_loss: 1.2064 - output1_accuracy: 0.7394 - output2_accuracy: 0.7104 - output3_accuracy: 0.5502 - val_loss: 5.6207 - val_output1_loss: 1.5993 - val_output2_loss: 1.8919 - val_output3_loss: 2.1295 - val_output1_accuracy: 0.5465 - val_output2_accuracy: 0.5243 - val_output3_accuracy: 0.3141\n",
            "Epoch 93/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8414 - output1_loss: 0.7267 - output2_loss: 0.9077 - output3_loss: 1.2069 - output1_accuracy: 0.7391 - output2_accuracy: 0.7108 - output3_accuracy: 0.5496 - val_loss: 5.6414 - val_output1_loss: 1.6163 - val_output2_loss: 1.8753 - val_output3_loss: 2.1498 - val_output1_accuracy: 0.5637 - val_output2_accuracy: 0.5464 - val_output3_accuracy: 0.3144\n",
            "Epoch 94/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8393 - output1_loss: 0.7259 - output2_loss: 0.9093 - output3_loss: 1.2041 - output1_accuracy: 0.7407 - output2_accuracy: 0.7113 - output3_accuracy: 0.5495 - val_loss: 5.7299 - val_output1_loss: 1.6356 - val_output2_loss: 1.9170 - val_output3_loss: 2.1773 - val_output1_accuracy: 0.5491 - val_output2_accuracy: 0.5358 - val_output3_accuracy: 0.3145\n",
            "Epoch 95/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8325 - output1_loss: 0.7245 - output2_loss: 0.9050 - output3_loss: 1.2030 - output1_accuracy: 0.7409 - output2_accuracy: 0.7125 - output3_accuracy: 0.5498 - val_loss: 5.6612 - val_output1_loss: 1.6222 - val_output2_loss: 1.8977 - val_output3_loss: 2.1413 - val_output1_accuracy: 0.5435 - val_output2_accuracy: 0.5306 - val_output3_accuracy: 0.3127\n",
            "Epoch 96/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8252 - output1_loss: 0.7208 - output2_loss: 0.9040 - output3_loss: 1.2004 - output1_accuracy: 0.7422 - output2_accuracy: 0.7127 - output3_accuracy: 0.5522 - val_loss: 5.7070 - val_output1_loss: 1.6479 - val_output2_loss: 1.9024 - val_output3_loss: 2.1568 - val_output1_accuracy: 0.5458 - val_output2_accuracy: 0.5301 - val_output3_accuracy: 0.3098\n",
            "Epoch 97/100\n",
            "2352/2352 [==============================] - 24s 10ms/step - loss: 2.8179 - output1_loss: 0.7189 - output2_loss: 0.9012 - output3_loss: 1.1979 - output1_accuracy: 0.7415 - output2_accuracy: 0.7122 - output3_accuracy: 0.5536 - val_loss: 5.7481 - val_output1_loss: 1.6555 - val_output2_loss: 1.9445 - val_output3_loss: 2.1480 - val_output1_accuracy: 0.5472 - val_output2_accuracy: 0.5275 - val_output3_accuracy: 0.3120\n",
            "Epoch 98/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8184 - output1_loss: 0.7209 - output2_loss: 0.8992 - output3_loss: 1.1982 - output1_accuracy: 0.7421 - output2_accuracy: 0.7137 - output3_accuracy: 0.5536 - val_loss: 5.6483 - val_output1_loss: 1.6077 - val_output2_loss: 1.8900 - val_output3_loss: 2.1505 - val_output1_accuracy: 0.5480 - val_output2_accuracy: 0.5285 - val_output3_accuracy: 0.3114\n",
            "Epoch 99/100\n",
            "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8164 - output1_loss: 0.7200 - output2_loss: 0.9000 - output3_loss: 1.1965 - output1_accuracy: 0.7413 - output2_accuracy: 0.7126 - output3_accuracy: 0.5535 - val_loss: 5.6035 - val_output1_loss: 1.6123 - val_output2_loss: 1.8799 - val_output3_loss: 2.1113 - val_output1_accuracy: 0.5462 - val_output2_accuracy: 0.5285 - val_output3_accuracy: 0.3123\n",
            "Epoch 100/100\n",
            "2352/2352 [==============================] - 22s 9ms/step - loss: 2.8078 - output1_loss: 0.7154 - output2_loss: 0.8974 - output3_loss: 1.1950 - output1_accuracy: 0.7444 - output2_accuracy: 0.7142 - output3_accuracy: 0.5533 - val_loss: 5.7564 - val_output1_loss: 1.6601 - val_output2_loss: 1.9321 - val_output3_loss: 2.1642 - val_output1_accuracy: 0.5461 - val_output2_accuracy: 0.5297 - val_output3_accuracy: 0.3075\n",
            "588/588 [==============================] - 4s 5ms/step - loss: 5.7564 - output1_loss: 1.6601 - output2_loss: 1.9321 - output3_loss: 2.1642 - output1_accuracy: 0.5461 - output2_accuracy: 0.5297 - output3_accuracy: 0.3075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Tahminleri alın\n",
        "predictions = model.predict(df_test)\n",
        "\n",
        "# Her bir çıkış için tahminleri ayırın\n",
        "predicted_output1 = predictions[0]\n",
        "predicted_output2 = predictions[1]\n",
        "predicted_output3 = predictions[2]\n",
        "\n",
        "\n",
        "# Tahmin sonuçlarını tam sayılara dönüştürme\n",
        "predicted_output1 = np.argmax(predicted_output1, axis=1)\n",
        "predicted_output2 = np.argmax(predicted_output2, axis=1)\n",
        "predicted_output3 = np.argmax(predicted_output3, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVaK8X4J3Xko",
        "outputId": "7b2d7fee-b091-4b42-b3d6-d0aefb59ec2d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "374/374 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(predicted_output1)):\n",
        "  binary_array = [0] * 9\n",
        "\n",
        "  binary_array[predicted_output1[i]] = 1\n",
        "  binary_array[predicted_output2[i]] = 1\n",
        "  binary_array[predicted_output3[i]] = 1\n",
        "\n",
        "  binary_string = ''.join(map(str, binary_array))\n",
        "  submission_sample.at[i, 'target'] = binary_string\n"
      ],
      "metadata": {
        "id": "pdR9jYl98Qed"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Lfu9HK_s_H2O",
        "outputId": "3bef299e-dd7e-4495-82e1-f968c110f76f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              id     target\n",
              "0      2e6105f5911256f4f6c4813ed  110001000\n",
              "1      c56ad71dae0a5dbd3e7d36adc  010001010\n",
              "2      4d02ea175f6581f0c6385311f  011001000\n",
              "3      3412d27a86c286ba078fa935c  010000101\n",
              "4      0203b561f6f7e10eafa46eefa  011001000\n",
              "...                          ...        ...\n",
              "11950  7687113f46112edf4f56666ee  011001000\n",
              "11951  5ff8eb7a06fd48b60dbc04f34  011001000\n",
              "11952  ac23a7b9ad3e5d61e738c854b  011001000\n",
              "11953  7da05018634ea2eee4b122756  011001000\n",
              "11954  57dbc6a230a851e6e2ee5c429  110001000\n",
              "\n",
              "[11955 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-720e16bf-9278-4e30-b76f-5547690d4f7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2e6105f5911256f4f6c4813ed</td>\n",
              "      <td>110001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c56ad71dae0a5dbd3e7d36adc</td>\n",
              "      <td>010001010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4d02ea175f6581f0c6385311f</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3412d27a86c286ba078fa935c</td>\n",
              "      <td>010000101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0203b561f6f7e10eafa46eefa</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11950</th>\n",
              "      <td>7687113f46112edf4f56666ee</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>5ff8eb7a06fd48b60dbc04f34</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11952</th>\n",
              "      <td>ac23a7b9ad3e5d61e738c854b</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11953</th>\n",
              "      <td>7da05018634ea2eee4b122756</td>\n",
              "      <td>011001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11954</th>\n",
              "      <td>57dbc6a230a851e6e2ee5c429</td>\n",
              "      <td>110001000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11955 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-720e16bf-9278-4e30-b76f-5547690d4f7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-720e16bf-9278-4e30-b76f-5547690d4f7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-720e16bf-9278-4e30-b76f-5547690d4f7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24c6046e-b801-4c67-ad6b-bffd5c7e8580\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24c6046e-b801-4c67-ad6b-bffd5c7e8580')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24c6046e-b801-4c67-ad6b-bffd5c7e8580 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path for the CSV file\n",
        "csv_file_path = 'submission_sample.csv'\n",
        "\n",
        "# Export the DataFrame to CSV\n",
        "submission_sample.to_csv(csv_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "Xsw2AS4iAunb"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dff['target'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WesqxZTEq1Nb",
        "outputId": "b120228c-6af0-469d-99b2-c3f752bfce91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dff = pd.read_csv(\"/content/submission_sample2.csv\")\n",
        "dff = dff.astype(str)\n",
        "\n",
        "\n",
        "def count_non_three_ones(df, column_name='target'):\n",
        "    count = 0\n",
        "    for target in df[column_name]:\n",
        "        if target.count('1') != 3:\n",
        "            count += 1\n",
        "            print(target)\n",
        "\n",
        "\n",
        "count_non_three_ones(dff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vqdJW8YqAcK",
        "outputId": "06b6ce2e-b565-4c35-b366-333b1000bed8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "1100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "1100\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "10000100\n",
            "10000100\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "100001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "1100\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "1010\n",
            "10001000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000001\n",
            "10100000\n",
            "1001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10001000\n",
            "1000001\n",
            "101000\n",
            "10000100\n",
            "10000100\n",
            "10100000\n",
            "100001000\n",
            "10001000\n",
            "100001\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "11000000\n",
            "1001000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10001000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001\n",
            "101000\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "100001\n",
            "10001000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "1100\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "100001\n",
            "10100000\n",
            "100001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "11000000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "1001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10001\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10000001\n",
            "10000100\n",
            "10100000\n",
            "11000000\n",
            "10001000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "1001000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "1100\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10000100\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "1001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "1000100\n",
            "10100000\n",
            "10000100\n",
            "110000000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "100001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "100001\n",
            "10001000\n",
            "10100000\n",
            "10000100\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "1100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10000100\n",
            "10000100\n",
            "10000100\n",
            "10100000\n",
            "10000100\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000100\n",
            "10100000\n",
            "1100\n",
            "10100000\n",
            "10100000\n",
            "1100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "100001\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10000100\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "110000000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "11000000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "1100\n",
            "10000100\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "1100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "1001000\n",
            "110000000\n",
            "10100000\n",
            "1001000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "110000000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "1001000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "1001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "110000000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "11000000\n",
            "10100000\n",
            "10000001\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "100001\n",
            "10001\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "110000000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000100\n",
            "10100000\n",
            "11000000\n",
            "10000100\n",
            "101000\n",
            "101000\n",
            "1001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "1001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001\n",
            "1010\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "1000100\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10000001\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10000100\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "100001\n",
            "101000\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "1100\n",
            "101000\n",
            "101000\n",
            "10000100\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "1001000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "1000100\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "100001\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "100001\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "100001\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "100001\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "100001\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000100\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10001000\n",
            "100001000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "1010\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "100000001\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10000100\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "1100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "11000000\n",
            "101000\n",
            "10001000\n",
            "10100000\n",
            "10000100\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "100001\n",
            "11000000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "100001\n",
            "10001000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "11000000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "100001\n",
            "1001000\n",
            "110000000\n",
            "10001000\n",
            "10100000\n",
            "10001000\n",
            "1001000\n",
            "10000001\n",
            "10100000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "101000\n",
            "10100000\n",
            "10001000\n",
            "10001000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "101000\n",
            "101000\n",
            "10100000\n",
            "10100000\n",
            "10001000\n",
            "10100000\n",
            "10100000\n",
            "10100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(neurons, input_shape, lr, num_classes, activation, loss):\n",
        "    try:\n",
        "        #Logger.info(\"Building LSTM Model...\")\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "        x = tf.keras.layers.Embedding(input_dim=5000, output_dim=10)(inputs)\n",
        "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons))(x)\n",
        "        # Define separate output layers for each target\n",
        "\n",
        "        output_1 = tf.keras.layers.Dense(num_classes, activation=activation, name=\"output_1\")(x)\n",
        "        output_2 = tf.keras.layers.Dense(num_classes, activation=activation, name=\"output_2\")(x)\n",
        "        output_3 = tf.keras.layers.Dense(num_classes, activation=activation, name=\"output_3\")(x)\n",
        "        # Define the model with multiple outputs\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=[ output_1, output_2, output_3])\n",
        "        model.compile(loss=loss, optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])\n",
        "        #Logger.debug(\"LSTM Model Built...\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"Exception while building model:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "X = df.drop([\"target1\", \"target2\", \"target3\"], axis=1)\n",
        "y = df[[\"target1\", \"target2\", \"target3\"]]\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "(X, x_test, y, y_test) = train_test_split(X, y, random_state=100, test_size=0.1, shuffle=True)\n",
        "\n",
        "\n",
        "(x_train, x_val, y_train, y_val) = train_test_split(X, y, random_state=100, test_size=0.1, shuffle=True)\n",
        "\n",
        "#Variables\n",
        "neurons = 100\n",
        "input_shape = 55\n",
        "lr = 0.001\n",
        "activation = 'softmax'\n",
        "loss = 'sparse_categorical_crossentropy'\n",
        "num_classes = 9 #number of unique classes per label\n",
        "\n",
        "model = build_lstm_model(neurons, input_shape, lr, num_classes, activation, loss)\n",
        "\n",
        "early_stopings = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=2, verbose=1, mode=\"auto\")\n",
        "callbacks = [early_stopings]\n",
        "\n",
        "trainHistrory = model.fit(\n",
        "    x=x_train,\n",
        "    y=[y_train[:, 0], y_train[:, 1], y_train[:, 2]],\n",
        "    validation_data=(x_val, [y_val[:, 0], y_val[:, 1], y_val[:, 2]]),\n",
        "    epochs= 10,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "#Evaluate on test data\n",
        "# x_test = test_df.drop([\"TARGET_0\", \"TARGET_1\", \"TARGET_2\", \"TARGET_3\"], axis=1)\n",
        "# y_test = test_df[[\"TARGET_0\", \"TARGET_1\", \"TARGET_2\", \"TARGET_3\"]]\n",
        "\n",
        "# y_test = np.array(y_test)\n",
        "\n",
        "test_accuracy = model.evaluate([x_test], [y_test[:, 0], y_test[:, 1], y_test[:, 2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-rZC3RrSnfS",
        "outputId": "fd74b756-d587-4d00-c637-a7f5ca75ca27"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2381/2381 [==============================] - 37s 14ms/step - loss: 4.1205 - output_1_loss: 1.0842 - output_2_loss: 1.3065 - output_3_loss: 1.7299 - output_1_sparse_categorical_accuracy: 0.6323 - output_2_sparse_categorical_accuracy: 0.6151 - output_3_sparse_categorical_accuracy: 0.2797 - val_loss: 4.0516 - val_output_1_loss: 1.0821 - val_output_2_loss: 1.2863 - val_output_3_loss: 1.6833 - val_output_1_sparse_categorical_accuracy: 0.6305 - val_output_2_sparse_categorical_accuracy: 0.6156 - val_output_3_sparse_categorical_accuracy: 0.3259\n",
            "Epoch 2/10\n",
            "2381/2381 [==============================] - 29s 12ms/step - loss: 4.0265 - output_1_loss: 1.0703 - output_2_loss: 1.2819 - output_3_loss: 1.6743 - output_1_sparse_categorical_accuracy: 0.6326 - output_2_sparse_categorical_accuracy: 0.6154 - output_3_sparse_categorical_accuracy: 0.3295 - val_loss: 4.0047 - val_output_1_loss: 1.0756 - val_output_2_loss: 1.2753 - val_output_3_loss: 1.6538 - val_output_1_sparse_categorical_accuracy: 0.6305 - val_output_2_sparse_categorical_accuracy: 0.6156 - val_output_3_sparse_categorical_accuracy: 0.3406\n",
            "Epoch 3/10\n",
            "2381/2381 [==============================] - 26s 11ms/step - loss: 3.9804 - output_1_loss: 1.0624 - output_2_loss: 1.2709 - output_3_loss: 1.6470 - output_1_sparse_categorical_accuracy: 0.6326 - output_2_sparse_categorical_accuracy: 0.6154 - output_3_sparse_categorical_accuracy: 0.3485 - val_loss: 3.9826 - val_output_1_loss: 1.0723 - val_output_2_loss: 1.2699 - val_output_3_loss: 1.6404 - val_output_1_sparse_categorical_accuracy: 0.6305 - val_output_2_sparse_categorical_accuracy: 0.6156 - val_output_3_sparse_categorical_accuracy: 0.3555\n",
            "Epoch 4/10\n",
            "2381/2381 [==============================] - 28s 12ms/step - loss: 3.9413 - output_1_loss: 1.0514 - output_2_loss: 1.2592 - output_3_loss: 1.6307 - output_1_sparse_categorical_accuracy: 0.6338 - output_2_sparse_categorical_accuracy: 0.6162 - output_3_sparse_categorical_accuracy: 0.3595 - val_loss: 3.9965 - val_output_1_loss: 1.0786 - val_output_2_loss: 1.2756 - val_output_3_loss: 1.6424 - val_output_1_sparse_categorical_accuracy: 0.6299 - val_output_2_sparse_categorical_accuracy: 0.6152 - val_output_3_sparse_categorical_accuracy: 0.3609\n",
            "Epoch 5/10\n",
            "2381/2381 [==============================] - 26s 11ms/step - loss: 3.9014 - output_1_loss: 1.0410 - output_2_loss: 1.2478 - output_3_loss: 1.6126 - output_1_sparse_categorical_accuracy: 0.6388 - output_2_sparse_categorical_accuracy: 0.6183 - output_3_sparse_categorical_accuracy: 0.3715 - val_loss: 3.9837 - val_output_1_loss: 1.0784 - val_output_2_loss: 1.2723 - val_output_3_loss: 1.6330 - val_output_1_sparse_categorical_accuracy: 0.6276 - val_output_2_sparse_categorical_accuracy: 0.6141 - val_output_3_sparse_categorical_accuracy: 0.3649\n",
            "Epoch 5: early stopping\n",
            "294/294 [==============================] - 3s 6ms/step - loss: 3.9955 - output_1_loss: 1.0750 - output_2_loss: 1.2840 - output_3_loss: 1.6365 - output_1_sparse_categorical_accuracy: 0.6266 - output_2_sparse_categorical_accuracy: 0.6087 - output_3_sparse_categorical_accuracy: 0.3607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = [np.argmax(pred, axis=-1) for pred in predictions]"
      ],
      "metadata": {
        "id": "3r9PmIrUm2W6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3hpKFYRrsvs",
        "outputId": "4f3afb33-66e8-4cee-9ed1-5577ce9b288a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 1, ..., 5, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scubLLmckQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = df.drop(columns=['target1', 'target2', 'target3']), df[['target1', 'target2', 'target3']]\n",
        "# Veri standartlaştırma\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_scaled,y, test_size=0.1, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wJBPSNXB7HIS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "B_0w7PFYums-",
        "outputId": "7f7e3d10-7e94-41e1-a719-6139e92dca06"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       target1  target2  target3\n",
              "65710        5        1        6\n",
              "82341        8        4        0\n",
              "33587        8        1        4\n",
              "323          5        1        2\n",
              "3840         5        6        2\n",
              "...        ...      ...      ...\n",
              "6265         5        1        0\n",
              "54886        5        1        2\n",
              "76820        5        1        6\n",
              "860          8        6        2\n",
              "15795        1        0        2\n",
              "\n",
              "[84644 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab039fc6-d03a-4637-98f2-35a843d5f9f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target1</th>\n",
              "      <th>target2</th>\n",
              "      <th>target3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>65710</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82341</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33587</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3840</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6265</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54886</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76820</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84644 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab039fc6-d03a-4637-98f2-35a843d5f9f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab039fc6-d03a-4637-98f2-35a843d5f9f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab039fc6-d03a-4637-98f2-35a843d5f9f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8df2847e-f5cb-4d02-a877-bd41ca4e4633\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8df2847e-f5cb-4d02-a877-bd41ca4e4633')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8df2847e-f5cb-4d02-a877-bd41ca4e4633 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWBtiNw-DN7V",
        "outputId": "dd48ff96-090d-45fd-f400-0ea8a924f847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.94010876,  0.02149488,  0.28148241, ..., -0.11106506,\n",
              "        -0.06259   , -0.45116505],\n",
              "       [ 0.94010876, -0.55099891,  1.49522509, ..., -0.11106506,\n",
              "        -0.06259   ,  2.21648375],\n",
              "       [-0.42545704, -0.33933236,  1.58461666, ..., -0.11106506,\n",
              "        -0.06259   ,  2.21648375],\n",
              "       ...,\n",
              "       [ 0.94010876, -0.40618411, -0.05337361, ..., -0.11106506,\n",
              "        -0.06259   , -0.45116505],\n",
              "       [-1.79102283,  0.98709599, -0.85689147, ..., -0.11106506,\n",
              "        -0.06259   ,  2.21648375],\n",
              "       [-0.42545704,  2.64279972, -0.88064255, ..., -0.11106506,\n",
              "        -0.06259   , -0.45116505]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# Input layer'ları oluşturun\n",
        "input1 = Input(shape=(max_sequence_length,))\n",
        "input2 = Input(shape=(max_sequence_length,))\n",
        "input3 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "# Embedding katmanlarını oluşturun\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length)\n",
        "\n",
        "# LSTM katmanlarını oluşturun\n",
        "lstm_layer = LSTM(64, return_sequences=True)\n",
        "\n",
        "# Her sütun için farklı çıkış katmanlarını oluşturun\n",
        "output1 = Dense(3, activation='softmax', name='output1')(lstm_layer(embedding_layer(input1)))\n",
        "output2 = Dense(3, activation='softmax', name='output2')(lstm_layer(embedding_layer(input2)))\n",
        "output3 = Dense(3, activation='softmax', name='output3')(lstm_layer(embedding_layer(input3)))\n",
        "\n",
        "# Modeli oluşturun\n",
        "model = Model(inputs=[input1, input2, input3], outputs=[output1, output2, output3])\n",
        "\n",
        "# Modeli derleyin\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Modeli eğitin\n",
        "model.fit([X_train_padded, X_train_padded, X_train_padded], [y_train1, y_train2, y_train3], validation_data=([X_test_padded, X_test_padded, X_test_padded], [y_test1, y_test2, y_test3]), epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "id": "qe8GQPrHIraj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sequential model with 3 layers\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(8, activation=\"relu\", name=\"layer1\"),\n",
        "        layers.Dense(6, activation=\"relu\", name=\"layer2\"),\n",
        "         LayerNormalization(axis=1),\n",
        "        layers.Dense(3, activation=\"relu\", name=\"layer3\"),\n",
        "        layers.Dense(3, activation=\"relu\", name=\"layer4\"),\n",
        "        Dropout(0.4),\n",
        "        layers.Dense(6, activation=\"relu\", name=\"layer5\"),\n",
        "        Dropout(0.4),\n",
        "         LayerNormalization(axis=1),\n",
        "        layers.Dense(8, activation=\"relu\", name=\"layer6\"),\n",
        "        layers.Dense(9, name=\"layer7\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Modeli Derleme\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x=X_train,y=y_train,\n",
        "                        epochs=150,\n",
        "                        validation_data = (X_test,y_test),\n",
        "                       callbacks=callbacks_list\n",
        "                        )"
      ],
      "metadata": {
        "id": "ElnL8Dfu30Rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b49cd7-b834-49c0-fe44-80c6ba655e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00096, saving model to weights-improvement-01-0.000957.hdf5\n",
            "2646/2646 [==============================] - 24s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 2/150\n",
            "  21/2646 [..............................] - ETA: 13s - loss: nan - accuracy: 0.0030    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 2: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 3/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 3: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 4/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 4: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 5/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 5: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 6/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 6: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 7/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 7: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 8/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 8: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 9/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 9: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 10/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 10: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 11/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 11: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 12/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 12: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 13/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 13: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 14/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 14: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 15/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 15: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 16/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 16: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 17/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 17: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 18/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 18: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 19/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 19: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 20/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 20: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 21/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 21: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 22/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 22: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 23/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 23: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 24/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 24: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 25/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 25: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 26/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 26: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 27/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 27: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 28/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 28: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 29/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 29: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 30/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 30: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 31/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 31: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 32/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 32: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 33/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 33: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 34/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 34: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 35/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 35: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 36/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 36: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 37/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 37: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 38/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 38: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 39/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 39: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 40/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 40: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 7ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 41/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 41: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 42/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 42: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 43/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 43: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 44/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 44: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 45/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 45: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 46/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 46: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 47/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 47: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 48/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 48: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 49/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 49: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 50/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 50: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 51/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 51: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 52/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 52: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 53/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 53: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 54/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 54: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 55/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 55: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 56/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 56: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 57/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 57: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 58/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 58: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 59/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 59: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 60/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 60: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 61/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 61: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 62/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 62: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 63/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 63: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 64/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 64: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 65/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 65: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 66/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 66: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 67/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 67: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 68/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 68: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 69/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 69: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 70/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 70: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 71/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 71: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 7ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 72/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 72: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 73/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 73: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 74/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 74: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 7ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 75/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 75: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 76/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 76: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 77/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 77: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 78/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 78: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 79/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 79: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 80/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 80: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 18s 7ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 81/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 81: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 82/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 82: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 83/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 83: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 84/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 84: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 85/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 85: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 86/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 86: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 87/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 87: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 88/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 88: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 89/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 89: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 90/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 90: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 91/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 91: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 92/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 92: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 93/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 93: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 94/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 94: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 95/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 95: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 96/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 96: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 97/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 97: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 98/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 98: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 99/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 99: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 100/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 100: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 101/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 101: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 102/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 102: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 103/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 103: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 104/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 104: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 105/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 105: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 106/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 106: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 107/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 107: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 108/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 108: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 109/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 109: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 110/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 110: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 111/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 111: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 112/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 112: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 113/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 113: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 114/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 114: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 115/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 115: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 116/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 116: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 117/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 117: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 118/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 118: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 119/150\n",
            "2642/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 119: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 120/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 120: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 121/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 121: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 122/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 122: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 123/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 123: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 124/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 124: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 125/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 125: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 126/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 126: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 127/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 127: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 128/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 128: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 129/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 129: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 130/150\n",
            "2636/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 130: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 131/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 131: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 132/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 132: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 133/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 133: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 134/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 134: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 15s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 135/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 135: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 136/150\n",
            "2643/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 136: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 137/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 137: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 138/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 138: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 139/150\n",
            "2641/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 139: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 140/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 140: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 141/150\n",
            "2639/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 141: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 142/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 142: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 143/150\n",
            "2640/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 143: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 144/150\n",
            "2646/2646 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 144: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 145/150\n",
            "2644/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 145: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 146/150\n",
            "2638/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 146: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 147/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 147: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 17s 7ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 148/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 148: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 149/150\n",
            "2637/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 149: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n",
            "Epoch 150/150\n",
            "2645/2646 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0012\n",
            "Epoch 150: val_accuracy did not improve from 0.00096\n",
            "2646/2646 [==============================] - 16s 6ms/step - loss: nan - accuracy: 0.0012 - val_loss: nan - val_accuracy: 9.5694e-04\n"
          ]
        }
      ]
    }
  ]
}